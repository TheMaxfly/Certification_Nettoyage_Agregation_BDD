{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb1e1bf",
   "metadata": {},
   "source": [
    "### Imports + chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9e22cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for root in [start, *start.parents]:\n",
    "        if (root / \"data\" / \"top_rated.json\").exists():\n",
    "            return root\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "\n",
    "# Source attendue dans ce repo : `data/top_rated.json`\n",
    "IN_JSON = PROJECT_ROOT / \"data\" / \"top_rated.json\"\n",
    "if not IN_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Fichier introuvable: {IN_JSON} (cwd={Path.cwd()})\")\n",
    "\n",
    "# Optionnel: limite pour itérer plus vite (None = tout)\n",
    "LIMIT_ROWS = 1000  # ex: 5000\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / \"exports\" / \"kitsu\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eaa42",
   "metadata": {},
   "source": [
    "### Charger + contrôler la structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bffb34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 1000\n",
      "meta keys: dict_keys(['category', 'source', 'endpoint', 'fetched_at', 'limit', 'offset'])\n",
      "sample keys: dict_keys(['id', 'slug', 'titles', 'status', 'synopsis', 'authors', 'ratings', 'popularity', 'tags'])\n"
     ]
    }
   ],
   "source": [
    "obj = json.loads(IN_JSON.read_text(encoding=\"utf-8\"))\n",
    "assert \"meta\" in obj and \"data\" in obj, \"Format inattendu (meta/data manquants)\"\n",
    "\n",
    "meta = obj[\"meta\"]\n",
    "data = obj[\"data\"]\n",
    "if LIMIT_ROWS is not None:\n",
    "    data = data[: int(LIMIT_ROWS)]\n",
    "print(\"rows:\", len(data))\n",
    "print(\"meta keys:\", meta.keys())\n",
    "print(\"sample keys:\", data[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7406bc",
   "metadata": {},
   "source": [
    "### Normaliser (flatten) vers un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cf140b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(data)\n",
    "\n",
    "# colonnes imbriquées attendues (selon ton échantillon)\n",
    "# titles.canonical / titles.en / titles.ja\n",
    "# ratings.average / ratings.rank\n",
    "# popularity.rank\n",
    "# tags.categories / tags.genres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec230d",
   "metadata": {},
   "source": [
    "### Cast types + champs “plats”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7883a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kitsu_id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "df[\"rating_average\"] = pd.to_numeric(df.get(\"ratings.average\"), errors=\"coerce\")\n",
    "df[\"rating_rank\"] = pd.to_numeric(df.get(\"ratings.rank\"), errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"popularity_rank\"] = pd.to_numeric(df.get(\"popularity.rank\"), errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "df[\"title_canonical\"] = df.get(\"titles.canonical\")\n",
    "df[\"title_en\"] = df.get(\"titles.en\")\n",
    "df[\"title_ja\"] = df.get(\"titles.ja\")\n",
    "\n",
    "df[\"status\"] = df.get(\"status\").astype(str).str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a11ab",
   "metadata": {},
   "source": [
    "### Nettoyage synopsis “RAG-friendly”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6574692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_synopsis(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\r\\n?\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    # Optionnel: enlever les mentions de source (souvent bruit pour RAG)\n",
    "    s = re.sub(r\"\\(Source:\\s*[^)]+\\)\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    return s or None\n",
    "\n",
    "df[\"synopsis_raw\"] = df.get(\"synopsis\")\n",
    "df[\"synopsis_clean\"] = df[\"synopsis_raw\"].map(clean_synopsis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a16e4d",
   "metadata": {},
   "source": [
    "### import json as _json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb96bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ensure_list(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    return x if isinstance(x, list) else []\n",
    "\n",
    "df[\"authors_json\"] = df.get(\"authors\").map(ensure_list)                 # list[dict{name,role}]\n",
    "df[\"categories_json\"] = df.get(\"tags.categories\").map(ensure_list)      # list[str]\n",
    "df[\"genres_json\"] = df.get(\"tags.genres\").map(ensure_list)              # list[str]\n",
    "\n",
    "# (optionnel) tag_total pour matching / recherche\n",
    "df[\"tags_all_json\"] = (df[\"categories_json\"] + df[\"genres_json\"]).map(lambda L: sorted(set([str(x).strip() for x in L if str(x).strip()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "344cb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as _json\n",
    "\n",
    "def ensure_list(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    return x if isinstance(x, list) else []\n",
    "\n",
    "df[\"authors_json\"] = df.get(\"authors\").map(ensure_list)                 # list[dict{name,role}]\n",
    "df[\"categories_json\"] = df.get(\"tags.categories\").map(ensure_list)      # list[str]\n",
    "df[\"genres_json\"] = df.get(\"tags.genres\").map(ensure_list)              # list[str]\n",
    "\n",
    "# (optionnel) tag_total pour matching / recherche\n",
    "df[\"tags_all_json\"] = (df[\"categories_json\"] + df[\"genres_json\"]).map(lambda L: sorted(set([str(x).strip() for x in L if str(x).strip()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e2b3a",
   "metadata": {},
   "source": [
    "### Dédup + contrôles qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecbc1971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 1000,\n",
       " 'synopsis_non_empty_%': np.float64(98.5),\n",
       " 'authors_non_empty_%': np.float64(0.0),\n",
       " 'tags_non_empty_%': np.float64(99.0)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"kitsu_id\"].notna()].copy()\n",
    "df = df.drop_duplicates(subset=[\"kitsu_id\"])\n",
    "\n",
    "kpi = {\n",
    "    \"rows\": len(df),\n",
    "    \"synopsis_non_empty_%\": (df[\"synopsis_clean\"].notna()).mean() * 100,\n",
    "    \"authors_non_empty_%\": (df[\"authors_json\"].map(len) > 0).mean() * 100,\n",
    "    \"tags_non_empty_%\": (df[\"tags_all_json\"].map(len) > 0).mean() * 100,\n",
    "}\n",
    "kpi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a380a29",
   "metadata": {},
   "source": [
    "### Sélection colonnes finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7333170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[[\n",
    "    \"kitsu_id\", \"slug\",\n",
    "    \"title_canonical\", \"title_en\", \"title_ja\",\n",
    "    \"status\",\n",
    "    \"synopsis_clean\", \"synopsis_raw\",\n",
    "    \"rating_average\", \"rating_rank\", \"popularity_rank\",\n",
    "    \"authors_json\",\n",
    "    \"categories_json\", \"genres_json\", \"tags_all_json\",\n",
    "]].copy()\n",
    "\n",
    "# ajouter meta (utile traçabilité)\n",
    "df_clean[\"kitsu_fetched_at\"] = meta.get(\"fetched_at\")\n",
    "df_clean[\"kitsu_endpoint\"] = meta.get(\"endpoint\")\n",
    "df_clean[\"kitsu_source\"] = meta.get(\"source\")\n",
    "df_clean[\"kitsu_category\"] = meta.get(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04811f",
   "metadata": {},
   "source": [
    "### Normalisation des clés de matching avec Manga Sanctuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "443e39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "def norm_title(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.replace(\"&\", \"and\")\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or None\n",
    "\n",
    "df_clean[\"title_norm_canonical\"] = df_clean[\"title_canonical\"].map(norm_title)\n",
    "df_clean[\"title_norm_en\"]        = df_clean[\"title_en\"].map(norm_title)\n",
    "df_clean[\"title_norm_ja\"]        = df_clean[\"title_ja\"].map(norm_title)\n",
    "\n",
    "def title_candidates(row):\n",
    "    cand = [row.get(\"title_canonical\"), row.get(\"title_en\"), row.get(\"title_ja\")]\n",
    "    cand = [c for c in cand if c and str(c).strip()]\n",
    "    # uniques, ordre conservé\n",
    "    out = []\n",
    "    for c in cand:\n",
    "        if c not in out:\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "df_clean[\"title_candidates_json\"] = df_clean.apply(title_candidates, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b3587",
   "metadata": {},
   "source": [
    "### nettoyage Synopsis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dd89808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_synopsis_plus(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\r\\n?\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    s = re.sub(r\"\\(Source:\\s*[^)]+\\)\", \"\", s, flags=re.I).strip()\n",
    "    s = re.sub(r\"(?im)^\\s*source:\\s*.*$\", \"\", s).strip()  # ligne \"Source: ...\"\n",
    "    return s or None\n",
    "\n",
    "df_clean[\"synopsis_clean\"] = df_clean[\"synopsis_raw\"].map(clean_synopsis_plus)\n",
    "df_clean[\"synopsis_len\"] = df_clean[\"synopsis_clean\"].fillna(\"\").str.len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abeb4a0",
   "metadata": {},
   "source": [
    "### gestion des auteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcf12511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"authors_count\"] = df_clean[\"authors_json\"].apply(lambda s: len(json.loads(s)) if isinstance(s,str) else (len(s) if isinstance(s,list) else 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0c2f4",
   "metadata": {},
   "source": [
    "### preparation Postgres JSONB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c452b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_list_to_text(s):\n",
    "    try:\n",
    "        L = json.loads(s) if isinstance(s, str) else s\n",
    "        if not isinstance(L, list): return \"\"\n",
    "        return \" | \".join([str(x).strip() for x in L if str(x).strip()])\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "df_clean[\"tags_all_text\"] = df_clean[\"tags_all_json\"].map(json_list_to_text)\n",
    "df_clean[\"genres_text\"]   = df_clean[\"genres_json\"].map(json_list_to_text)\n",
    "df_clean[\"categories_text\"] = df_clean[\"categories_json\"].map(json_list_to_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa04ea6",
   "metadata": {},
   "source": [
    "### normalisation des notes sur 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a7e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"rating_average_10\"] = (pd.to_numeric(df_clean[\"rating_average\"], errors=\"coerce\") / 10).round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41cceb",
   "metadata": {},
   "source": [
    "### Export systématique CSV + Parquet (JSONB-ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "199297be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: /home/maxime/python/certification/preparation_bdd/exports/kitsu/kitsu_top_rated_clean.csv /home/maxime/python/certification/preparation_bdd/exports/kitsu/kitsu_top_rated_clean.parquet\n"
     ]
    }
   ],
   "source": [
    "def export_both_jsonb_ready(df: pd.DataFrame, csv_path: Path, parq_path: Path, json_cols: list[str]):\n",
    "    # Parquet: garde les listes/dicts (nécessite `pyarrow` ou `fastparquet`)\n",
    "    try:\n",
    "        df.to_parquet(parq_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Parquet export skipped: {e!r} (installe pyarrow pour l'activer)\")\n",
    "\n",
    "    # CSV: sérialise JSONB en string JSON\n",
    "    df_csv = df.copy()\n",
    "    for c in json_cols:\n",
    "        if c in df_csv.columns:\n",
    "            df_csv[c] = df_csv[c].apply(lambda x: _json.dumps(x, ensure_ascii=False) if isinstance(x, (list, dict)) else \"[]\")\n",
    "    df_csv.to_csv(csv_path, index=False)\n",
    "\n",
    "OUT_CSV = OUT_DIR / \"kitsu_top_rated_clean.csv\"\n",
    "OUT_PARQ = OUT_DIR / \"kitsu_top_rated_clean.parquet\"\n",
    "\n",
    "export_both_jsonb_ready(\n",
    "    df_clean,\n",
    "    OUT_CSV,\n",
    "    OUT_PARQ,\n",
    "    json_cols=[\"authors_json\",\"categories_json\",\"genres_json\",\"tags_all_json\"]\n",
    ")\n",
    "\n",
    "print(\"Exported:\", OUT_CSV, OUT_PARQ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
