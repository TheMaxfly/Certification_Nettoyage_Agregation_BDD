{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b962daf0",
      "metadata": {},
      "source": [
        "# Nettoyage — Source 1 (Manga Sanctuary Volumes)\n",
        "\n",
        "Objectif (C3) : produire une base **joinable** propre à partir de `manga_sanctuary_volumes*.jsonl`.\n",
        "\n",
        "Sorties :\n",
        "- `ms_series_clean`\n",
        "- `ms_volumes_clean`\n",
        "\n",
        "Étapes :\n",
        "1) lecture robuste + `__line__` + rejets\n",
        "2) normalisation types (id, numéro, url)\n",
        "3) nettoyage textes (`series_synopsis`, `volume_synopsis`)\n",
        "4) dédup (series_id, volume_url)\n",
        "5) suppression des colonnes `.1` si duplicats (ex: `volume_url.1`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a3d835",
      "metadata": {},
      "source": [
        "## nettoyage de manga_sanctuary_volumes.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6657c4",
      "metadata": {},
      "source": [
        "### Setup et Chemins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9568d8c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, re, ast\n",
        "from pathlib import Path\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dépendances Parquet (export systématique Parquet + CSV)\n",
        "\n",
        "Ce notebook exporte **toujours** en **CSV** et tente d'exporter en **Parquet**.\n",
        "Pour garantir Parquet, on installe `pyarrow` si nécessaire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PARQUET_READY = True\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import sys\n",
        "\n",
        "def ensure_pyarrow():\n",
        "    try:\n",
        "        import pyarrow  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Installation via pip (nécessite un environnement avec accès aux paquets)\n",
        "    try:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow\"])\n",
        "        import pyarrow  # noqa: F401\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Impossible d'installer pyarrow automatiquement :\", repr(e))\n",
        "        print(\"➡️ Le CSV sera bien produit, mais Parquet peut échouer tant que pyarrow/fastparquet n'est pas installé.\")\n",
        "        return False\n",
        "\n",
        "PARQUET_READY = ensure_pyarrow()\n",
        "print(\"PARQUET_READY =\", PARQUET_READY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "501d2831",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAW_PATH = /home/maxime/python/certification/preparation_bdd/data/manga_sanctuary_volumes (1).jsonl\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "# Si notebook lancé depuis un sous-dossier, remonter d'un niveau si besoin\n",
        "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "def find_raw_jsonl(project_root: Path) -> Path:\n",
        "    \"\"\"Trouve le fichier volumes JSONL dans data/ ou dans /mnt/data (selon l'environnement).\"\"\"\n",
        "    candidates = [\n",
        "        project_root / \"data\" / \"manga_sanctuary_volumes (1).jsonl\",\n",
        "        project_root / \"data\" / \"manga_sanctuary_volumes.jsonl\",\n",
        "        project_root / \"data\" / \"manga_sanctuary_volumes(1).jsonl\",\n",
        "        # environnements type sandbox\n",
        "        Path(\"/mnt/data/manga_sanctuary_volumes (1).jsonl\"),\n",
        "        Path(\"/mnt/data/manga_sanctuary_volumes.jsonl\"),\n",
        "        Path(\"/mnt/data/manga_sanctuary_volumes(1).jsonl\"),\n",
        "        # fallback éventuel\n",
        "        Path(\"/data/manga_sanctuary_volumes (1).jsonl\"),\n",
        "        Path(\"/data/manga_sanctuary_volumes.jsonl\"),\n",
        "        Path(\"/data/manga_sanctuary_volumes(1).jsonl\"),\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            return p\n",
        "\n",
        "    # fallback: premier jsonl contenant 'volumes' dans data/\n",
        "    data_dir = project_root / \"data\"\n",
        "    if data_dir.exists():\n",
        "        for p in sorted(data_dir.glob(\"*volumes*.jsonl\")):\n",
        "            return p\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Impossible de trouver manga_sanctuary_volumes*.jsonl (cherché dans data/, /mnt/data, /data).\"\n",
        "    )\n",
        "\n",
        "RAW_PATH = find_raw_jsonl(PROJECT_ROOT)\n",
        "print(\"RAW_PATH =\", RAW_PATH)\n",
        "\n",
        "OUT_DIR  = PROJECT_ROOT / \"out_ms_staging\"\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# sorties parquet\n",
        "CLEAN_VOLUMES_PATH = OUT_DIR / \"ms_volumes_clean.parquet\"\n",
        "CLEAN_SERIES_PATH  = OUT_DIR / \"ms_series_clean.parquet\"\n",
        "\n",
        "# sorties CSV (toujours)\n",
        "VOLUMES_CSV_PATH = OUT_DIR / \"ms_volumes_clean.csv\"\n",
        "SERIES_CSV_PATH  = OUT_DIR / \"ms_series_clean.csv\"\n",
        "\n",
        "# audit\n",
        "REJECTED_PATH = OUT_DIR / \"ms_volumes_rejected.jsonl\"\n",
        "STATS_PATH    = OUT_DIR / \"ms_volumes_stats.json\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cd38c9",
      "metadata": {},
      "source": [
        "### Lecture et rejet des lignes invalides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1a180562",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid_rows: 89188\n",
            "rejected_rows: 0\n"
          ]
        }
      ],
      "source": [
        "valid_rows = []\n",
        "rejected = []\n",
        "\n",
        "with RAW_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            rejected.append({\"line\": i, \"reason\": \"empty_line\"})\n",
        "            continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            obj[\"__line__\"] = i  # traçabilité audit\n",
        "            valid_rows.append(obj)\n",
        "        except json.JSONDecodeError:\n",
        "            rejected.append({\"line\": i, \"reason\": \"invalid_json\"})\n",
        "\n",
        "print(\"valid_rows:\", len(valid_rows))\n",
        "print(\"rejected_rows:\", len(rejected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e9f971",
      "metadata": {},
      "source": [
        "### DataFrame + vue  avant nettoyage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "8356f22d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (89188, 39)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>series_id</th>\n",
              "      <th>series_url</th>\n",
              "      <th>series_title</th>\n",
              "      <th>series_type</th>\n",
              "      <th>series_category</th>\n",
              "      <th>series_year</th>\n",
              "      <th>series_other_titles</th>\n",
              "      <th>series_dessinateur</th>\n",
              "      <th>series_scenariste</th>\n",
              "      <th>series_genres</th>\n",
              "      <th>...</th>\n",
              "      <th>volume_country</th>\n",
              "      <th>volume_status</th>\n",
              "      <th>volume_tomes_published</th>\n",
              "      <th>volume_tomes_total</th>\n",
              "      <th>volume_members_rating</th>\n",
              "      <th>volume_members_votes</th>\n",
              "      <th>volume_experts_rating</th>\n",
              "      <th>volume_experts_votes</th>\n",
              "      <th>volume_synopsis</th>\n",
              "      <th>__line__</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78152</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manga/7815...</td>\n",
              "      <td>Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...</td>\n",
              "      <td>Manga</td>\n",
              "      <td>2019</td>\n",
              "      <td>2019</td>\n",
              "      <td>[刀剣乱舞-ONLINE- アンソロジー ~本丸爛漫日和~]</td>\n",
              "      <td>Kyôko MAKI</td>\n",
              "      <td>Niko WAKUHARA</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>pays édition</td>\n",
              "      <td>Complète</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>刀剣男士達を華麗に描く公式アンソロジー! 本丸でのほのぼのエピソードから、合戦場でのかっこい...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12139</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>Blast</td>\n",
              "      <td>Manhwa</td>\n",
              "      <td>Sonyun</td>\n",
              "      <td>2007</td>\n",
              "      <td>[]</td>\n",
              "      <td>Kangho PARK</td>\n",
              "      <td>Ha na LEE</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12139</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>Blast</td>\n",
              "      <td>Manhwa</td>\n",
              "      <td>Sonyun</td>\n",
              "      <td>2007</td>\n",
              "      <td>[]</td>\n",
              "      <td>Kangho PARK</td>\n",
              "      <td>Ha na LEE</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  series_id                                         series_url  \\\n",
              "0     78152  https://www.manga-sanctuary.com/bdd/manga/7815...   \n",
              "1     12139  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "2     12139  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "\n",
              "                                        series_title series_type  \\\n",
              "0  Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...       Manga   \n",
              "1                                              Blast      Manhwa   \n",
              "2                                              Blast      Manhwa   \n",
              "\n",
              "  series_category series_year             series_other_titles  \\\n",
              "0            2019        2019  [刀剣乱舞-ONLINE- アンソロジー ~本丸爛漫日和~]   \n",
              "1          Sonyun        2007                              []   \n",
              "2          Sonyun        2007                              []   \n",
              "\n",
              "  series_dessinateur series_scenariste series_genres  ... volume_country  \\\n",
              "0         Kyôko MAKI     Niko WAKUHARA            []  ...   pays édition   \n",
              "1        Kangho PARK         Ha na LEE            []  ...            NaN   \n",
              "2        Kangho PARK         Ha na LEE            []  ...            NaN   \n",
              "\n",
              "  volume_status volume_tomes_published  volume_tomes_total  \\\n",
              "0      Complète                    1.0                 1.0   \n",
              "1           NaN                    NaN                 NaN   \n",
              "2           NaN                    NaN                 NaN   \n",
              "\n",
              "   volume_members_rating  volume_members_votes  volume_experts_rating  \\\n",
              "0                    NaN                   0.0                    NaN   \n",
              "1                    NaN                   0.0                    NaN   \n",
              "2                    NaN                   0.0                    NaN   \n",
              "\n",
              "   volume_experts_votes                                    volume_synopsis  \\\n",
              "0                   0.0  刀剣男士達を華麗に描く公式アンソロジー! 本丸でのほのぼのエピソードから、合戦場でのかっこい...   \n",
              "1                   0.0                                                      \n",
              "2                   0.0                                                      \n",
              "\n",
              "  __line__  \n",
              "0        1  \n",
              "1        2  \n",
              "2        3  \n",
              "\n",
              "[3 rows x 39 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw = pd.json_normalize(valid_rows)\n",
        "print(\"shape:\", df_raw.shape)\n",
        "df_raw.head(3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ce9da7",
      "metadata": {},
      "source": [
        "### Règles “champs obligatoires” (et rejets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1282478a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kept_after_required: 89129\n",
            "rejected_after_required: 59\n"
          ]
        }
      ],
      "source": [
        "REQUIRED = [\"series_id\", \"series_url\", \"series_title\", \"volume_url\", \"volume_title\"]\n",
        "\n",
        "# ligne invalide si un champ requis est NA ou si volume_title est vide\n",
        "missing_mask = df_raw[REQUIRED].isna().any(axis=1) | (df_raw[\"volume_title\"].fillna(\"\").astype(str).str.strip() == \"\")\n",
        "\n",
        "df_bad = df_raw[missing_mask].copy()\n",
        "df_ok  = df_raw[~missing_mask].copy()\n",
        "\n",
        "# log détaillé des raisons + line d'origine\n",
        "for _, row in df_bad.iterrows():\n",
        "    reasons = []\n",
        "    for c in REQUIRED:\n",
        "        v = row.get(c)\n",
        "        if pd.isna(v) or (isinstance(v, str) and v.strip() == \"\"):\n",
        "            reasons.append(f\"missing_{c}\")\n",
        "    rejected.append({\n",
        "        \"line\": int(row[\"__line__\"]) if pd.notna(row.get(\"__line__\")) else None,\n",
        "        \"reason\": \"missing_required_fields\",\n",
        "        \"reasons\": reasons\n",
        "    })\n",
        "\n",
        "print(\"kept_after_required:\", df_ok.shape[0])\n",
        "print(\"rejected_after_required:\", df_bad.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da47523",
      "metadata": {},
      "source": [
        "### homogénéisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "52cdf4f3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "series_id                    Int64\n",
              "series_url                  object\n",
              "series_title                object\n",
              "series_type                 object\n",
              "series_category             object\n",
              "series_year                  Int64\n",
              "series_other_titles         object\n",
              "series_dessinateur          object\n",
              "series_scenariste           object\n",
              "series_genres               object\n",
              "series_tags                 object\n",
              "series_mag_prepub           object\n",
              "series_statuses             object\n",
              "series_popularity_rank       Int64\n",
              "series_members_rating      Float64\n",
              "series_members_votes         Int64\n",
              "series_experts_rating      Float64\n",
              "series_experts_votes         Int64\n",
              "series_synopsis             object\n",
              "series_related_works        object\n",
              "volume_url                  object\n",
              "volume_title                object\n",
              "volume_number                Int64\n",
              "volume_publication_date     object\n",
              "volume_dessinateur          object\n",
              "volume_scenariste           object\n",
              "volume_editeur              object\n",
              "volume_format               object\n",
              "volume_pages                 Int64\n",
              "volume_country              object\n",
              "volume_status               object\n",
              "volume_tomes_published     float64\n",
              "volume_tomes_total         float64\n",
              "volume_members_rating      float64\n",
              "volume_members_votes       float64\n",
              "volume_experts_rating      float64\n",
              "volume_experts_votes       float64\n",
              "volume_synopsis             object\n",
              "dtype: object"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def to_int(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "def to_float(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Float64\")\n",
        "\n",
        "df = df_ok.copy()\n",
        "\n",
        "df[\"series_id\"] = to_int(df[\"series_id\"])\n",
        "df[\"series_year\"] = to_int(df.get(\"series_year\"))\n",
        "df[\"series_popularity_rank\"] = to_int(df.get(\"series_popularity_rank\"))\n",
        "\n",
        "df[\"series_members_rating\"] = to_float(df.get(\"series_members_rating\"))\n",
        "df[\"series_experts_rating\"] = to_float(df.get(\"series_experts_rating\"))\n",
        "df[\"series_members_votes\"] = to_int(df.get(\"series_members_votes\"))\n",
        "df[\"series_experts_votes\"] = to_int(df.get(\"series_experts_votes\"))\n",
        "\n",
        "df[\"volume_number\"] = to_int(df.get(\"volume_number\"))\n",
        "df[\"volume_pages\"]  = to_int(df.get(\"volume_pages\"))\n",
        "\n",
        "# règle métier : 0 pages => null\n",
        "df.loc[df[\"volume_pages\"] == 0, \"volume_pages\"] = pd.NA\n",
        "\n",
        "df.dtypes.head(38)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2491ee8",
      "metadata": {},
      "source": [
        "### Nettoyage texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d9fc041b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>series_title</th>\n",
              "      <th>volume_title</th>\n",
              "      <th>series_url</th>\n",
              "      <th>volume_url</th>\n",
              "      <th>series_synopsis</th>\n",
              "      <th>volume_synopsis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...</td>\n",
              "      <td>Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manga/7815...</td>\n",
              "      <td>https://www.manga-sanctuary.com/manga-touken-r...</td>\n",
              "      <td>None</td>\n",
              "      <td>刀剣男士達を華麗に描く公式アンソロジー! 本丸でのほのぼのエピソードから、合戦場でのかっこい...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blast</td>\n",
              "      <td>Blast 5 (Haksan)</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>https://www.manga-sanctuary.com/manhwa-blast-v...</td>\n",
              "      <td>Kael (alias Ryu Shiho) était une menace sous s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Blast</td>\n",
              "      <td>Blast 4 (Haksan)</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>https://www.manga-sanctuary.com/manhwa-blast-v...</td>\n",
              "      <td>Kael (alias Ryu Shiho) était une menace sous s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Blast</td>\n",
              "      <td>Blast 3 (Haksan)</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>https://www.manga-sanctuary.com/manhwa-blast-v...</td>\n",
              "      <td>Kael (alias Ryu Shiho) était une menace sous s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Blast</td>\n",
              "      <td>Blast 2 (Haksan)</td>\n",
              "      <td>https://www.manga-sanctuary.com/bdd/manhwa/121...</td>\n",
              "      <td>https://www.manga-sanctuary.com/manhwa-blast-v...</td>\n",
              "      <td>Kael (alias Ryu Shiho) était une menace sous s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        series_title  \\\n",
              "0  Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...   \n",
              "1                                              Blast   \n",
              "2                                              Blast   \n",
              "3                                              Blast   \n",
              "4                                              Blast   \n",
              "\n",
              "                                        volume_title  \\\n",
              "0  Touken ranbu -ONLINE- Anthology ~ Honmaru Ranm...   \n",
              "1                                   Blast 5 (Haksan)   \n",
              "2                                   Blast 4 (Haksan)   \n",
              "3                                   Blast 3 (Haksan)   \n",
              "4                                   Blast 2 (Haksan)   \n",
              "\n",
              "                                          series_url  \\\n",
              "0  https://www.manga-sanctuary.com/bdd/manga/7815...   \n",
              "1  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "2  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "3  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "4  https://www.manga-sanctuary.com/bdd/manhwa/121...   \n",
              "\n",
              "                                          volume_url  \\\n",
              "0  https://www.manga-sanctuary.com/manga-touken-r...   \n",
              "1  https://www.manga-sanctuary.com/manhwa-blast-v...   \n",
              "2  https://www.manga-sanctuary.com/manhwa-blast-v...   \n",
              "3  https://www.manga-sanctuary.com/manhwa-blast-v...   \n",
              "4  https://www.manga-sanctuary.com/manhwa-blast-v...   \n",
              "\n",
              "                                     series_synopsis  \\\n",
              "0                                               None   \n",
              "1  Kael (alias Ryu Shiho) était une menace sous s...   \n",
              "2  Kael (alias Ryu Shiho) était une menace sous s...   \n",
              "3  Kael (alias Ryu Shiho) était une menace sous s...   \n",
              "4  Kael (alias Ryu Shiho) était une menace sous s...   \n",
              "\n",
              "                                     volume_synopsis  \n",
              "0  刀剣男士達を華麗に描く公式アンソロジー! 本丸でのほのぼのエピソードから、合戦場でのかっこい...  \n",
              "1                                               None  \n",
              "2                                               None  \n",
              "3                                               None  \n",
              "4                                               None  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_text(x):\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return None\n",
        "    s = str(x).replace(\"\\u00a0\", \" \").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    # placeholders -> null\n",
        "    if s.lower() in {\"na\", \"n/a\", \"none\", \"\"}:\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "TEXT_COLS = [\n",
        "    # titres / urls (join keys)\n",
        "    \"series_title\", \"series_url\",\n",
        "    \"volume_title\", \"volume_url\",\n",
        "\n",
        "    # synopsis (important RAG)\n",
        "    \"series_synopsis\", \"volume_synopsis\",\n",
        "\n",
        "    # autres champs texte\n",
        "    \"series_mag_prepub\",\n",
        "    \"volume_country\", \"volume_format\",\n",
        "]\n",
        "\n",
        "for c in TEXT_COLS:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].map(clean_text)\n",
        "\n",
        "df[[c for c in [\"series_title\",\"volume_title\",\"series_url\",\"volume_url\",\"series_synopsis\",\"volume_synopsis\"] if c in df.columns]].head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4291ac37",
      "metadata": {},
      "source": [
        "### nettoyage series_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "63f1aaa5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "non-null clean%: 97.43\n",
            "year_guess non-null%: 2.48\n",
            "allowed%: 97.43\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "series_category_clean\n",
              "Shonen          32981\n",
              "Seinen          29643\n",
              "Shojo           13843\n",
              "Yaoi             4485\n",
              "Josei            3413\n",
              "Ecchi-Hentai     1240\n",
              "Kodomo            412\n",
              "Shonen-Aï         339\n",
              "Yuri              314\n",
              "Parodie           106\n",
              "Shojo-Aï           62\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# 1) Mapping de normalisation (variantes -> canonique)\n",
        "MAP_CAT = {\n",
        "    \"sonyun\": \"Shonen\",\n",
        "    \"shounen\": \"Shonen\",\n",
        "    \"sunjung\": \"Shojo\",\n",
        "    \"chungnyun\": \"Seinen\",\n",
        "}\n",
        "\n",
        "# 2) Catégories autorisées (contrôle qualité)\n",
        "ALLOWED = {\n",
        "    \"Shonen\", \"Seinen\", \"Shojo\", \"Josei\", \"Kodomo\",\n",
        "    \"Yaoi\", \"Yuri\", \"Shonen-Aï\", \"Shojo-Aï\",\n",
        "    \"Ecchi-Hentai\", \"Parodie\"\n",
        "}\n",
        "\n",
        "def _norm_key(x: str) -> str:\n",
        "    \"\"\"Normalise pour comparaison : trim + minuscules + sans accents + espaces simples.\"\"\"\n",
        "    x = str(x).strip()\n",
        "    x = unicodedata.normalize(\"NFKD\", x)\n",
        "    x = \"\".join(ch for ch in x if not unicodedata.combining(ch))\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip().lower()\n",
        "    return x\n",
        "\n",
        "def extract_year(x):\n",
        "    \"\"\"Récupère une année si la catégorie est en fait une année.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return pd.NA\n",
        "    s = str(x).strip()\n",
        "    if s.isdigit():\n",
        "        n = int(s)\n",
        "        if 1900 <= n <= 2100:\n",
        "            return n\n",
        "    return pd.NA\n",
        "\n",
        "def clean_category(x):\n",
        "    \"\"\"Nettoie la catégorie : supprime années/vides, mappe variantes, contrôle les valeurs.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\":\n",
        "        return None\n",
        "\n",
        "    # années -> None (mais récupérées dans series_category_year_guess)\n",
        "    y = extract_year(s)\n",
        "    if pd.notna(y):\n",
        "        return None\n",
        "\n",
        "    key = _norm_key(s)\n",
        "    canon = MAP_CAT.get(key, s.strip())\n",
        "\n",
        "    # normalise un peu la casse/hyphens (optionnel)\n",
        "    canon = canon.replace(\"Ai\", \"Aï\").replace(\"Shojo-Ai\", \"Shojo-Aï\").replace(\"Shonen-Ai\", \"Shonen-Aï\")\n",
        "\n",
        "    # contrôle : si hors liste, on garde mais on le marquera en \"other\"\n",
        "    return canon\n",
        "\n",
        "# Colonnes résultantes\n",
        "df[\"series_category_year_guess\"] = df[\"series_category\"].map(extract_year).astype(\"Int64\")\n",
        "df[\"series_category_clean\"] = df[\"series_category\"].map(clean_category)\n",
        "\n",
        "# Flag qualité (utile en C3)\n",
        "df[\"series_category_is_allowed\"] = df[\"series_category_clean\"].isin(ALLOWED)\n",
        "\n",
        "# KPI / preuve\n",
        "print(\"non-null clean%:\", round((df[\"series_category_clean\"].notna().mean()) * 100, 2))\n",
        "print(\"year_guess non-null%:\", round((df[\"series_category_year_guess\"].notna().mean()) * 100, 2))\n",
        "print(\"allowed%:\", round((df[\"series_category_is_allowed\"].mean()) * 100, 2))\n",
        "\n",
        "df[\"series_category_clean\"].value_counts().head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fbc937",
      "metadata": {},
      "source": [
        "### Normalisation des listes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "5160578c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "series_genres non-empty rate = 0.0\n",
            "series_tags non-empty rate = 0.0\n"
          ]
        }
      ],
      "source": [
        "def clean_list(v):\n",
        "    \"\"\"Normalise en liste de strings propres.\n",
        "- None/NaN -> []\n",
        "- str -> [str]\n",
        "- list -> liste nettoyée\n",
        "- dédoublonnage case-insensitive\n",
        "\"\"\"\n",
        "    if v is None or (isinstance(v, float) and pd.isna(v)):\n",
        "        return []\n",
        "    if isinstance(v, str):\n",
        "        v = [v]\n",
        "    if not isinstance(v, list):\n",
        "        return []\n",
        "\n",
        "    out = []\n",
        "    for item in v:\n",
        "        item = clean_text(item)\n",
        "        if item:\n",
        "            out.append(item)\n",
        "\n",
        "    seen = set()\n",
        "    out2 = []\n",
        "    for x in out:\n",
        "        k = x.lower()\n",
        "        if k not in seen:\n",
        "            seen.add(k)\n",
        "            out2.append(x)\n",
        "    return out2\n",
        "\n",
        "LIST_COLS = [\"series_other_titles\", \"series_genres\", \"series_tags\"]\n",
        "for c in LIST_COLS:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].map(clean_list)\n",
        "\n",
        "# KPI (preuve)\n",
        "for c in [\"series_genres\", \"series_tags\"]:\n",
        "    if c in df.columns:\n",
        "        print(c, \"non-empty rate =\", round((df[c].map(len) > 0).mean(), 6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa7487a",
      "metadata": {},
      "source": [
        "### Déduplication volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "6c0d8445",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before: 89129 after: 89129\n"
          ]
        }
      ],
      "source": [
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"volume_url\"], keep=\"first\").copy()\n",
        "after = len(df)\n",
        "print(\"before:\", before, \"after:\", after)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d66807",
      "metadata": {},
      "source": [
        "### Split “Series” vs “Volumes”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2bcfd205",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ms_series: (13208, 23)\n",
            "ms_volumes: (89129, 19)\n",
            "series columns unique? True\n",
            "volumes columns unique? True\n"
          ]
        }
      ],
      "source": [
        "def unique_cols(cols):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(c)\n",
        "    return out\n",
        "\n",
        "SERIES_COLS = unique_cols([c for c in df.columns if c.startswith(\"series_\")])\n",
        "VOLUME_COLS = unique_cols([c for c in df.columns if c.startswith(\"volume_\")] + [\"series_id\"])\n",
        "\n",
        "ms_series = df[SERIES_COLS].drop_duplicates(subset=[\"series_id\"]).copy()\n",
        "ms_volumes = df[VOLUME_COLS].drop_duplicates(subset=[\"volume_url\"]).copy()\n",
        "\n",
        "print(\"ms_series:\", ms_series.shape)\n",
        "print(\"ms_volumes:\", ms_volumes.shape)\n",
        "print(\"series columns unique?\", ms_series.columns.is_unique)\n",
        "print(\"volumes columns unique?\", ms_volumes.columns.is_unique)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nettoyage des colonnes doublonnées (ex: `volume_url` / `volume_url.1`)\n",
        "\n",
        "Certains exports CSV peuvent introduire des colonnes suffixées `.1` lorsque des noms sont dupliqués.\n",
        "On supprime `X.1` si elle est identique à `X`, sinon on la renomme en `X_alt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate-columns cleanup (series): {'dropped': [], 'renamed': {}}\n",
            "Duplicate-columns cleanup (volumes): {'dropped': [], 'renamed': {}}\n",
            "volume_url.1 present? False\n"
          ]
        }
      ],
      "source": [
        "def drop_duplicate_dot1_cols(df):\n",
        "    \"\"\"Supprime les colonnes suffixées '.1' si elles dupliquent une colonne base.\n",
        "    Si la base existe mais la valeur diffère, renomme en '<base>_alt'.\n",
        "    Retourne (df, info).\n",
        "    \"\"\"\n",
        "    dropped = []\n",
        "    renamed = {}\n",
        "    cols = list(df.columns)\n",
        "    for c in cols:\n",
        "        if not isinstance(c, str) or not c.endswith('.1'):\n",
        "            continue\n",
        "        base = c[:-2]\n",
        "        if base in df.columns:\n",
        "            try:\n",
        "                same = df[base].equals(df[c])\n",
        "            except Exception:\n",
        "                same = False\n",
        "            if same:\n",
        "                dropped.append(c)\n",
        "            else:\n",
        "                new = f\"{base}_alt\"\n",
        "                i = 2\n",
        "                while new in df.columns or new in renamed.values():\n",
        "                    new = f\"{base}_alt{i}\"\n",
        "                    i += 1\n",
        "                renamed[c] = new\n",
        "        else:\n",
        "            renamed[c] = base\n",
        "\n",
        "    if renamed:\n",
        "        df = df.rename(columns=renamed)\n",
        "    if dropped:\n",
        "        df = df.drop(columns=dropped)\n",
        "\n",
        "    return df, {\"dropped\": dropped, \"renamed\": renamed}\n",
        "\n",
        "ms_series, _dup_series = drop_duplicate_dot1_cols(ms_series)\n",
        "ms_volumes, _dup_volumes = drop_duplicate_dot1_cols(ms_volumes)\n",
        "\n",
        "print(\"Duplicate-columns cleanup (series):\", _dup_series)\n",
        "print(\"Duplicate-columns cleanup (volumes):\", _dup_volumes)\n",
        "\n",
        "assert ms_series.columns.is_unique, \"Colonnes dupliquées dans ms_series\"\n",
        "assert ms_volumes.columns.is_unique, \"Colonnes dupliquées dans ms_volumes\"\n",
        "\n",
        "# (optionnel) vérification spécifique\n",
        "print(\"volume_url.1 present?\", \"volume_url.1\" in ms_volumes.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234e2631",
      "metadata": {},
      "source": [
        "### Controles qualité KPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "985922db",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'volume_url_unique': True,\n",
              " 'series_id_null_rate': 0.0,\n",
              " 'volume_number_null_rate': 0.05765800132392375,\n",
              " 'volume_pages_null_rate': 0.5108438330958498,\n",
              " 'series_count': 13208,\n",
              " 'volumes_count': 89129,\n",
              " 'avg_volumes_per_series': 6.7481}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qc = {\n",
        "    \"volume_url_unique\": bool(ms_volumes[\"volume_url\"].is_unique),\n",
        "    \"series_id_null_rate\": float(ms_volumes[\"series_id\"].isna().mean()),\n",
        "    \"volume_number_null_rate\": float(ms_volumes[\"volume_number\"].isna().mean()) if \"volume_number\" in ms_volumes else None,\n",
        "    \"volume_pages_null_rate\": float(ms_volumes[\"volume_pages\"].isna().mean()) if \"volume_pages\" in ms_volumes else None,\n",
        "    \"series_count\": int(ms_series.shape[0]),\n",
        "    \"volumes_count\": int(ms_volumes.shape[0]),\n",
        "    \"avg_volumes_per_series\": float(round(ms_volumes.shape[0] / ms_series.shape[0], 4)) if ms_series.shape[0] else None,\n",
        "}\n",
        "qc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Préparation des exports Postgres JSONB + écriture des rejets\n",
        "\n",
        "- Les colonnes de type **liste / dict** doivent être sérialisées en **JSON valide** pour pouvoir être castées en `jsonb` dans Postgres.\n",
        "- On conserve les DataFrames `ms_series` / `ms_volumes` tels quels pour les étapes suivantes, et on prépare des copies `*_pg` dédiées à l’export CSV.\n",
        "- On écrit aussi le fichier `ms_volumes_rejected.jsonl` (audit C3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rejected_written_to: /home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_volumes_rejected.jsonl count: 59\n"
          ]
        }
      ],
      "source": [
        "import json as pyjson\n",
        "\n",
        "def jsonb_ready_copy(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Retourne une copie du DF où toutes les colonnes list/dict sont sérialisées en JSON valide.\"\"\"\n",
        "    out = df.copy()\n",
        "    for col in out.columns:\n",
        "        # test rapide sur quelques valeurs non nulles\n",
        "        sample = out[col].dropna().head(20)\n",
        "        if len(sample) == 0:\n",
        "            continue\n",
        "        if any(isinstance(v, (list, dict)) for v in sample):\n",
        "            out[col] = out[col].map(lambda v: pyjson.dumps(v if isinstance(v, (list, dict)) else [], ensure_ascii=False))\n",
        "    return out\n",
        "\n",
        "# évite les colonnes redondantes *_json si la colonne de base est déjà une list\n",
        "for base in [\"series_other_titles\", \"series_statuses\", \"series_related_works\"]:\n",
        "    jcol = base + \"_json\"\n",
        "    if base in ms_series.columns and jcol in ms_series.columns:\n",
        "        # si la base est déjà une list, on supprime la colonne _json\n",
        "        try:\n",
        "            if isinstance(ms_series.loc[ms_series.index[0], base], list):\n",
        "                ms_series = ms_series.drop(columns=[jcol])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "ms_series_pg  = jsonb_ready_copy(ms_series)\n",
        "ms_volumes_pg = jsonb_ready_copy(ms_volumes)\n",
        "\n",
        "# audit rejets (C3)\n",
        "# 'rejected' contient déjà les rejets JSON invalides + les rejets sur champs requis\n",
        "with REJECTED_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for r in rejected:\n",
        "        f.write(pyjson.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"rejected_written_to:\", REJECTED_PATH, \"count:\", len(rejected))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "53a2a250",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "INT_COLS = [\"volume_number\",\"volume_pages\",\"volume_members_votes\",\"volume_experts_votes\",\n",
        "            \"volume_tomes_published\",\"volume_tomes_total\"]\n",
        "FLOAT_COLS = [\"volume_members_rating\",\"volume_experts_rating\"]\n",
        "\n",
        "for c in INT_COLS:\n",
        "    if c in ms_volumes.columns:\n",
        "        ms_volumes[c] = pd.to_numeric(ms_volumes[c], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "for c in FLOAT_COLS:\n",
        "    if c in ms_volumes.columns:\n",
        "        ms_volumes[c] = pd.to_numeric(ms_volumes[c], errors=\"coerce\").astype(\"Float64\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3708ac",
      "metadata": {},
      "source": [
        "### Exports + stats + rejets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'csv': {'series': '/home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_series_clean.csv',\n",
              "  'volumes': '/home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_volumes_clean.csv'},\n",
              " 'parquet': {'series': '/home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_series_clean.parquet',\n",
              "  'volumes': '/home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_volumes_clean.parquet'},\n",
              " 'parquet_errors': {},\n",
              " 'rejected_jsonl': '/home/maxime/python/certification/preparation_bdd/out_ms_staging/ms_volumes_rejected.jsonl'}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json as pyjson\n",
        "\n",
        "# garde-fous (évite l'erreur pyarrow \"Duplicate column names\")\n",
        "assert ms_series.columns.is_unique, \"Colonnes dupliquées dans ms_series\"\n",
        "assert ms_volumes.columns.is_unique, \"Colonnes dupliquées dans ms_volumes\"\n",
        "\n",
        "# 1) CSV (systématique) — version JSONB-ready\n",
        "ms_series_pg.to_csv(SERIES_CSV_PATH, index=False)\n",
        "ms_volumes_pg.to_csv(VOLUMES_CSV_PATH, index=False)\n",
        "\n",
        "# 2) Parquet (systématique) — garde les types natifs si possible\n",
        "parquet_errors = {}\n",
        "try:\n",
        "    ms_series.to_parquet(CLEAN_SERIES_PATH, index=False)\n",
        "except Exception as e:\n",
        "    parquet_errors[\"ms_series\"] = repr(e)\n",
        "\n",
        "try:\n",
        "    ms_volumes.to_parquet(CLEAN_VOLUMES_PATH, index=False)\n",
        "except Exception as e:\n",
        "    parquet_errors[\"ms_volumes\"] = repr(e)\n",
        "\n",
        "# stats C3 (audit + preuve)\n",
        "stats_obj = {\n",
        "    \"raw_valid_rows\": int(len(valid_rows)),\n",
        "    \"raw_rejected_rows\": int(sum(1 for r in rejected if r.get(\"reason\") in {\"empty_line\", \"invalid_json\"})),\n",
        "    \"rejected_total\": int(len(rejected)),\n",
        "    \"kept_after_required_fields\": int(df_ok.shape[0]) if \"df_ok\" in globals() else None,\n",
        "    \"series_after_dedup\": int(ms_series.shape[0]),\n",
        "    \"volumes_after_dedup\": int(ms_volumes.shape[0]),\n",
        "    \"qc\": qc if \"qc\" in globals() else None,\n",
        "    \"kpi_lists\": kpi_lists if \"kpi_lists\" in globals() else None,\n",
        "    \"exports\": {\n",
        "        \"csv\": {\"series\": str(SERIES_CSV_PATH), \"volumes\": str(VOLUMES_CSV_PATH)},\n",
        "        \"parquet\": {\"series\": str(CLEAN_SERIES_PATH), \"volumes\": str(CLEAN_VOLUMES_PATH)},\n",
        "        \"parquet_errors\": parquet_errors,\n",
        "        \"rejected_jsonl\": str(REJECTED_PATH),\n",
        "    },\n",
        "}\n",
        "\n",
        "STATS_PATH.write_text(pyjson.dumps(stats_obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "stats_obj[\"exports\"]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
