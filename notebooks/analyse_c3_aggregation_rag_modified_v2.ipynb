{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C3 — Agrégation & exports RAG-friendly (version modifiée)\n",
        "\n",
        "Modifs :\n",
        "- Suppression de `volume_members_rating` (et variante `volume_membrers_rating`) : pas utile pour ton conseiller.\n",
        "- `review_count` dans `ms_volumes_enriched` casté en **Int64** et rempli à **0** pour les volumes sans review.\n",
        "\n",
        "Entrées (`out_ms_staging/`) :\n",
        "- `ms_series_clean.csv`\n",
        "- `ms_volumes_clean.csv`\n",
        "- `ms_reviews_clean.csv`\n",
        "- (optionnel) `ms_reviews_rag_ready.csv`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed48fd24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except Exception:\n",
        "    load_dotenv = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39de198",
      "metadata": {},
      "source": [
        "## 0) Parquet (CSV + Parquet systématiques)\n",
        "\n",
        "Dépendance requise : `pyarrow` (voir `requirements-dev.txt`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41737679",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_pyarrow_or_fail():\n",
        "    try:\n",
        "        import pyarrow  # noqa: F401\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"Parquet requis pour ce notebook, mais pyarrow n'est pas disponible. \"\n",
        "            \"Installe-le manuellement (ex: pip install -r requirements-dev.txt). \"\n",
        "            f\"Détail: {repr(e)}\"\n",
        "        )\n",
        "\n",
        "PARQUET_READY = ensure_pyarrow_or_fail()\n",
        "print(\"PARQUET_READY =\", PARQUET_READY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85415721",
      "metadata": {},
      "source": [
        "## 1) Chemins\n",
        "\n",
        "Par défaut :\n",
        "- entrées : `out_ms_staging/`\n",
        "- sorties : `out_ms_final/`\n",
        "\n",
        "Override possible via variables d'environnement (ou `.env`) :\n",
        "- `STAGING_DIR=...`\n",
        "- `OUT_DIR=...`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968aef77",
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIRED_STAGING_FILES = [\n",
        "    \"ms_series_clean.csv\",\n",
        "    \"ms_volumes_clean.csv\",\n",
        "    \"ms_reviews_clean.csv\",\n",
        "]\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for root in [start, *start.parents]:\n",
        "        if (root / \"pyproject.toml\").exists():\n",
        "            return root\n",
        "    return start\n",
        "\n",
        "def staging_has_required(staging: Path) -> bool:\n",
        "    return staging.exists() and all((staging / f).exists() for f in REQUIRED_STAGING_FILES)\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "PROJECT_ROOT_RESOLVED = PROJECT_ROOT.resolve()\n",
        "\n",
        "if load_dotenv is not None:\n",
        "    load_dotenv(PROJECT_ROOT / \".env\", override=False)\n",
        "\n",
        "def resolve_from_root(p: str) -> Path:\n",
        "    candidate = Path(p).expanduser()\n",
        "    if candidate.is_absolute():\n",
        "        return candidate\n",
        "    return (PROJECT_ROOT / candidate).resolve()\n",
        "\n",
        "def try_rel(p: Path) -> str:\n",
        "    try:\n",
        "        return str(p.resolve().relative_to(PROJECT_ROOT_RESOLVED))\n",
        "    except Exception:\n",
        "        return str(p)\n",
        "\n",
        "staging_env = os.getenv(\"STAGING_DIR\")\n",
        "out_env = os.getenv(\"OUT_DIR\")\n",
        "\n",
        "STAGING = resolve_from_root(staging_env) if staging_env else (PROJECT_ROOT / \"out_ms_staging\")\n",
        "if not staging_has_required(STAGING):\n",
        "    # Utile si le kernel démarre dans `notebooks/`\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for root in [cwd, *cwd.parents]:\n",
        "        candidate = root / \"out_ms_staging\"\n",
        "        if staging_has_required(candidate):\n",
        "            STAGING = candidate\n",
        "            break\n",
        "    else:\n",
        "        # Compat Colab\n",
        "        mnt = Path(\"/mnt/data\")\n",
        "        if staging_has_required(mnt):\n",
        "            STAGING = mnt\n",
        "\n",
        "if not staging_has_required(STAGING):\n",
        "    raise FileNotFoundError(\n",
        "        \"Fichiers staging manquants. \"\n",
        "        f\"Attendu dans STAGING={STAGING}: {REQUIRED_STAGING_FILES}. \"\n",
        "        \"Tu peux forcer le chemin via STAGING_DIR (env/.env).\"\n",
        "    )\n",
        "\n",
        "SERIES_CSV  = STAGING / \"ms_series_clean.csv\"\n",
        "VOLUMES_CSV = STAGING / \"ms_volumes_clean.csv\"\n",
        "REVIEWS_CSV = STAGING / \"ms_reviews_clean.csv\"\n",
        "RAGREV_CSV  = STAGING / \"ms_reviews_rag_ready.csv\"  # optionnel\n",
        "\n",
        "OUT_DIR = resolve_from_root(out_env) if out_env else (PROJECT_ROOT / \"out_ms_final\")\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "SERIES_ENR_CSV   = OUT_DIR / \"ms_series_enriched.csv\"\n",
        "VOLUMES_ENR_CSV  = OUT_DIR / \"ms_volumes_enriched.csv\"\n",
        "SERIES_MET_CSV   = OUT_DIR / \"ms_series_metrics.csv\"\n",
        "VOLUMES_MET_CSV  = OUT_DIR / \"ms_volume_metrics.csv\"\n",
        "\n",
        "SERIES_ENR_PARQ  = OUT_DIR / \"ms_series_enriched.parquet\"\n",
        "VOLUMES_ENR_PARQ = OUT_DIR / \"ms_volumes_enriched.parquet\"\n",
        "SERIES_MET_PARQ  = OUT_DIR / \"ms_series_metrics.parquet\"\n",
        "VOLUMES_MET_PARQ = OUT_DIR / \"ms_volume_metrics.parquet\"\n",
        "\n",
        "ORPHAN_REVIEWS_CSV = OUT_DIR / \"ms_reviews_orphan_volume_url.csv\"\n",
        "STATS_JSON         = OUT_DIR / \"ms_c3_stats.json\"\n",
        "\n",
        "RAG_REVIEWS_JSONL  = OUT_DIR / \"rag_reviews.jsonl\"\n",
        "RAG_VPROF_JSONL    = OUT_DIR / \"rag_volume_profiles.jsonl\"\n",
        "\n",
        "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
        "print(\"STAGING =\", try_rel(STAGING))\n",
        "print(\"OUT_DIR =\", try_rel(OUT_DIR))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512d0764",
      "metadata": {},
      "source": [
        "## 2) Chargement (nullable dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628204b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_csv_nullable(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, dtype_backend=\"numpy_nullable\")\n",
        "\n",
        "def require_columns(df: pd.DataFrame, cols: list[str], name: str) -> None:\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"{name}: colonnes manquantes: {missing}\")\n",
        "\n",
        "ms_series  = read_csv_nullable(SERIES_CSV)\n",
        "ms_volumes = read_csv_nullable(VOLUMES_CSV)\n",
        "ms_reviews = read_csv_nullable(REVIEWS_CSV)\n",
        "\n",
        "require_columns(ms_series, [\"series_id\"], \"ms_series\")\n",
        "require_columns(ms_volumes, [\"volume_url\", \"series_id\"], \"ms_volumes\")\n",
        "require_columns(ms_reviews, [\"review_url\", \"volume_url\", \"series_id\"], \"ms_reviews\")\n",
        "\n",
        "print(\"ms_series :\", ms_series.shape)\n",
        "print(\"ms_volumes:\", ms_volumes.shape)\n",
        "print(\"ms_reviews:\", ms_reviews.shape)\n",
        "\n",
        "if ms_series[\"series_id\"].isna().any():\n",
        "    raise ValueError(\"ms_series: series_id contient des valeurs nulles\")\n",
        "if ms_volumes[\"volume_url\"].isna().any():\n",
        "    raise ValueError(\"ms_volumes: volume_url contient des valeurs nulles\")\n",
        "if ms_reviews[\"volume_url\"].isna().any():\n",
        "    raise ValueError(\"ms_reviews: volume_url contient des valeurs nulles\")\n",
        "\n",
        "if not ms_series[\"series_id\"].is_unique:\n",
        "    raise ValueError(\"ms_series: series_id n'est pas unique\")\n",
        "if not ms_volumes[\"volume_url\"].is_unique:\n",
        "    raise ValueError(\"ms_volumes: volume_url n'est pas unique\")\n",
        "if not ms_reviews[\"review_url\"].is_unique:\n",
        "    raise ValueError(\"ms_reviews: review_url n'est pas unique\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77be7693",
      "metadata": {},
      "source": [
        "## 3) Audit orphelins (volume_url absent des volumes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "571bdecb",
      "metadata": {},
      "outputs": [],
      "source": [
        "orphans = ms_reviews.loc[~ms_reviews[\"volume_url\"].isin(ms_volumes[\"volume_url\"])].copy()\n",
        "print(\"orphan_reviews_rows =\", len(orphans))\n",
        "if len(orphans):\n",
        "    orphans.to_csv(ORPHAN_REVIEWS_CSV, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f706311",
      "metadata": {},
      "source": [
        "## 4) KPI par volume & par série\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b59c6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"review_score\" in ms_reviews.columns:\n",
        "    ms_reviews[\"review_score\"] = pd.to_numeric(ms_reviews[\"review_score\"], errors=\"coerce\").astype(\"Float64\")\n",
        "else:\n",
        "    ms_reviews[\"review_score\"] = pd.Series([pd.NA] * len(ms_reviews), dtype=\"Float64\")\n",
        "\n",
        "ms_reviews[\"review_date_dt\"] = pd.to_datetime(ms_reviews.get(\"review_date_iso\"), errors=\"coerce\")\n",
        "ms_reviews[\"has_date\"] = ms_reviews[\"review_date_dt\"].notna()\n",
        "if \"review_body\" in ms_reviews.columns:\n",
        "    body = ms_reviews[\"review_body\"].astype(\"string\")\n",
        "    ms_reviews[\"has_body\"] = body.notna() & body.str.strip().ne(\"\")\n",
        "else:\n",
        "    ms_reviews[\"has_body\"] = False\n",
        "\n",
        "g = ms_reviews.groupby(\"volume_url\", dropna=False)\n",
        "volume_metrics = g.agg(\n",
        "    review_count=(\"review_url\", \"count\"),\n",
        "    score_mean=(\"review_score\", \"mean\"),\n",
        "    score_median=(\"review_score\", \"median\"),\n",
        "    score_min=(\"review_score\", \"min\"),\n",
        "    score_max=(\"review_score\", \"max\"),\n",
        "    with_body_count=(\"has_body\", \"sum\"),\n",
        "    with_date_count=(\"has_date\", \"sum\"),\n",
        "    first_review_date=(\"review_date_dt\", \"min\"),\n",
        "    last_review_date=(\"review_date_dt\", \"max\"),\n",
        ").reset_index()\n",
        "\n",
        "volume_metrics[\"with_body_pct\"] = (volume_metrics[\"with_body_count\"] / volume_metrics[\"review_count\"] * 100).round(2)\n",
        "volume_metrics[\"with_date_pct\"] = (volume_metrics[\"with_date_count\"] / volume_metrics[\"review_count\"] * 100).round(2)\n",
        "volume_metrics[\"first_review_date_iso\"] = volume_metrics[\"first_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "volume_metrics[\"last_review_date_iso\"]  = volume_metrics[\"last_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "gs = ms_reviews.groupby(\"series_id\", dropna=False)\n",
        "series_metrics = gs.agg(\n",
        "    series_review_count=(\"review_url\", \"count\"),\n",
        "    series_score_mean=(\"review_score\", \"mean\"),\n",
        "    series_score_median=(\"review_score\", \"median\"),\n",
        "    series_score_min=(\"review_score\", \"min\"),\n",
        "    series_score_max=(\"review_score\", \"max\"),\n",
        "    series_with_body_count=(\"has_body\", \"sum\"),\n",
        "    series_with_date_count=(\"has_date\", \"sum\"),\n",
        "    series_first_review_date=(\"review_date_dt\", \"min\"),\n",
        "    series_last_review_date=(\"review_date_dt\", \"max\"),\n",
        ").reset_index()\n",
        "\n",
        "series_metrics[\"series_with_body_pct\"] = (series_metrics[\"series_with_body_count\"] / series_metrics[\"series_review_count\"] * 100).round(2)\n",
        "series_metrics[\"series_with_date_pct\"] = (series_metrics[\"series_with_date_count\"] / series_metrics[\"series_review_count\"] * 100).round(2)\n",
        "series_metrics[\"series_first_review_date_iso\"] = series_metrics[\"series_first_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "series_metrics[\"series_last_review_date_iso\"]  = series_metrics[\"series_last_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "volume_metrics.head(3), series_metrics.head(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a5c166",
      "metadata": {},
      "source": [
        "## 5) Enrichissement + suppression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fb28ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# volumes enrichis\n",
        "ms_volumes_enriched = ms_volumes.merge(\n",
        "    volume_metrics.drop(columns=[\"first_review_date\", \"last_review_date\"]),\n",
        "    on=\"volume_url\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "# ✅ suppression colonne(s) non pertinente(s) (orthographes / variations possibles)\n",
        "drop_cols = []\n",
        "for c in ms_volumes_enriched.columns:\n",
        "    cl = str(c).strip().lower()\n",
        "    if cl in {\"volume_members_rating\", \"volume_membrers_rating\"}:\n",
        "        drop_cols.append(c)\n",
        "    elif \"members_rating\" in cl or \"membrers_rating\" in cl:\n",
        "        drop_cols.append(c)\n",
        "\n",
        "drop_cols = sorted(set(drop_cols))\n",
        "if drop_cols:\n",
        "    ms_volumes_enriched = ms_volumes_enriched.drop(columns=drop_cols)\n",
        "print(\"Dropped volume columns:\", drop_cols)\n",
        "\n",
        "# ✅ compteurs volumes -> Int64 + fill 0 (volumes sans reviews)\n",
        "for c in [\"review_count\", \"with_body_count\", \"with_date_count\"]:\n",
        "    if c in ms_volumes_enriched.columns:\n",
        "        ms_volumes_enriched[c] = (\n",
        "            pd.to_numeric(ms_volumes_enriched[c], errors=\"coerce\")\n",
        "              .fillna(0)\n",
        "              .astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "print(ms_volumes_enriched[[\"review_count\", \"with_body_count\", \"with_date_count\"]].dtypes)\n",
        "ms_volumes_enriched[[\"volume_url\", \"series_id\", \"volume_number\", \"review_count\"]].head(5)\n",
        "\n",
        "# séries enrichies\n",
        "vol_count = ms_volumes.groupby(\"series_id\").size().reset_index(name=\"series_volume_count\")\n",
        "ms_series_enriched = (\n",
        "    ms_series\n",
        "    .merge(vol_count, on=\"series_id\", how=\"left\")\n",
        "    .merge(\n",
        "        series_metrics.drop(columns=[\"series_first_review_date\", \"series_last_review_date\"]),\n",
        "        on=\"series_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# ✅ compteurs KPI série -> Int64 + fill 0 (séries sans reviews)\n",
        "for c in [\"series_review_count\", \"series_with_body_count\", \"series_with_date_count\"]:\n",
        "    if c in ms_series_enriched.columns:\n",
        "        ms_series_enriched[c] = (\n",
        "            pd.to_numeric(ms_series_enriched[c], errors=\"coerce\")\n",
        "              .fillna(0)\n",
        "              .astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "print(ms_series_enriched[[\"series_review_count\", \"series_with_body_count\", \"series_with_date_count\"]].dtypes)\n",
        "ms_series_enriched[[\"series_id\", \"series_review_count\", \"series_volume_count\"]].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67902373",
      "metadata": {},
      "source": [
        "### gestion float64 / int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ebf231c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "for c in [\"series_year\", \"series_category_year_guess\"]:\n",
        "    if c in ms_series_enriched.columns:\n",
        "        ms_series_enriched[c] = pd.to_numeric(ms_series_enriched[c], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "if \"series_review_count\" in ms_series_enriched.columns:\n",
        "    ms_series_enriched[\"series_review_count\"] = (\n",
        "        pd.to_numeric(ms_series_enriched[\"series_review_count\"], errors=\"coerce\")\n",
        "          .fillna(0)\n",
        "          .astype(\"Int64\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53478d29",
      "metadata": {},
      "source": [
        "## 6) Exports (CSV + Parquet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25504baa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_both(df: pd.DataFrame, csv_path: Path, parq_path: Path):\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_parquet(parq_path, index=False)\n",
        "\n",
        "export_both(ms_volumes_enriched, VOLUMES_ENR_CSV, VOLUMES_ENR_PARQ)\n",
        "export_both(ms_series_enriched, SERIES_ENR_CSV, SERIES_ENR_PARQ)\n",
        "export_both(volume_metrics, VOLUMES_MET_CSV, VOLUMES_MET_PARQ)\n",
        "export_both(series_metrics, SERIES_MET_CSV, SERIES_MET_PARQ)\n",
        "\n",
        "print(\"Exported:\", VOLUMES_ENR_CSV.name, SERIES_ENR_CSV.name, VOLUMES_MET_CSV.name, SERIES_MET_CSV.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec0d3e7",
      "metadata": {},
      "source": [
        "## 7) RAG docs (reviews + volume profiles) — chunking optionnel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd9934d",
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_CHUNKING = False\n",
        "CHUNK_CHARS = 1200\n",
        "OVERLAP_CHARS = 200\n",
        "\n",
        "def chunk_text(s: str, chunk_chars=1200, overlap=200):\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return []\n",
        "    if len(s) <= chunk_chars:\n",
        "        return [s]\n",
        "    out, start = [], 0\n",
        "    while start < len(s):\n",
        "        end = min(len(s), start + chunk_chars)\n",
        "        out.append(s[start:end])\n",
        "        if end == len(s):\n",
        "            break\n",
        "        start = max(0, end - overlap)\n",
        "    return out\n",
        "\n",
        "if RAGREV_CSV.exists():\n",
        "    rag_df = read_csv_nullable(RAGREV_CSV)\n",
        "elif \"rag_ready\" in ms_reviews.columns:\n",
        "    rag_df = ms_reviews.loc[ms_reviews[\"rag_ready\"] == True].copy()\n",
        "else:\n",
        "    rag_df = ms_reviews.copy()\n",
        "\n",
        "if \"rag_text\" not in rag_df.columns:\n",
        "    if \"review_body\" in rag_df.columns:\n",
        "        rag_df[\"rag_text\"] = rag_df[\"review_body\"]\n",
        "    else:\n",
        "        rag_df[\"rag_text\"] = pd.NA\n",
        "\n",
        "if \"rag_len\" not in rag_df.columns:\n",
        "    rag_df[\"rag_len\"] = rag_df[\"rag_text\"].astype(\"string\").str.len()\n",
        "\n",
        "need_cols = [\"review_url\",\"series_id\",\"volume_url\",\"volume_number\",\"review_score\",\"review_date_iso\",\"review_author\",\"review_type\",\"rag_text\",\"rag_len\"]\n",
        "for c in need_cols:\n",
        "    if c not in rag_df.columns:\n",
        "        rag_df[c] = pd.NA\n",
        "\n",
        "# rag_reviews.jsonl\n",
        "rag_docs = []\n",
        "for _, r in rag_df.iterrows():\n",
        "    rid = \"\" if pd.isna(r[\"review_url\"]) else str(r[\"review_url\"]).strip()\n",
        "    if not rid or rid.lower() in {\"nan\", \"none\"}:\n",
        "        continue\n",
        "    text = r[\"rag_text\"]\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        continue\n",
        "    chunks = chunk_text(str(text), CHUNK_CHARS, OVERLAP_CHARS) if USE_CHUNKING else [str(text)]\n",
        "    for ci, chunk in enumerate(chunks):\n",
        "        doc_id = f\"{rid}#c{ci}\" if USE_CHUNKING else rid\n",
        "        md_ = {\n",
        "            \"review_url\": rid,\n",
        "            \"series_id\": None if pd.isna(r[\"series_id\"]) else int(r[\"series_id\"]),\n",
        "            \"volume_url\": None if pd.isna(r[\"volume_url\"]) else str(r[\"volume_url\"]),\n",
        "            \"volume_number\": None if pd.isna(r[\"volume_number\"]) else int(r[\"volume_number\"]),\n",
        "            \"review_score\": None if pd.isna(r[\"review_score\"]) else float(r[\"review_score\"]),\n",
        "            \"review_date_iso\": None if pd.isna(r[\"review_date_iso\"]) else str(r[\"review_date_iso\"]),\n",
        "            \"review_author\": None if pd.isna(r[\"review_author\"]) else str(r[\"review_author\"]),\n",
        "            \"review_type\": None if pd.isna(r[\"review_type\"]) else str(r[\"review_type\"]),\n",
        "            \"chunk_index\": ci if USE_CHUNKING else None,\n",
        "        }\n",
        "        rag_docs.append({\"id\": doc_id, \"text\": chunk, \"metadata\": md_})\n",
        "\n",
        "with RAG_REVIEWS_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for d in rag_docs:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"rag_reviews_docs_written =\", len(rag_docs))\n",
        "\n",
        "# rag_volume_profiles.jsonl\n",
        "reviews_for_snippets = rag_df.copy()\n",
        "reviews_for_snippets[\"review_score\"] = pd.to_numeric(reviews_for_snippets[\"review_score\"], errors=\"coerce\").astype(\"Float64\")\n",
        "reviews_for_snippets[\"rag_text\"] = reviews_for_snippets[\"rag_text\"].astype(\"string\").fillna(\"\")\n",
        "\n",
        "def safe_str(v) -> str:\n",
        "    if v is None:\n",
        "        return \"\"\n",
        "    try:\n",
        "        if pd.isna(v):\n",
        "            return \"\"\n",
        "    except Exception:\n",
        "        return str(v)\n",
        "    return str(v)\n",
        "\n",
        "def build_volume_profile(row):\n",
        "    vol_title = safe_str(row.get(\"volume_title\")).strip()\n",
        "    vol_num = row.get(\"volume_number\")\n",
        "    series_id = row.get(\"series_id\")\n",
        "    vol_url = safe_str(row.get(\"volume_url\")).strip()\n",
        "\n",
        "    vol_syn = safe_str(row.get(\"volume_synopsis\")).strip()\n",
        "    if vol_syn.lower() in {\"nan\", \"none\"}:\n",
        "        vol_syn = \"\"\n",
        "\n",
        "    syn_series = \"\"\n",
        "    if pd.notna(series_id):\n",
        "        srow = ms_series_enriched.loc[ms_series_enriched[\"series_id\"] == series_id]\n",
        "        if len(srow):\n",
        "            syn_series = safe_str(srow.iloc[0].get(\"series_synopsis\")).strip()\n",
        "            if syn_series.lower() in {\"nan\", \"none\"}:\n",
        "                syn_series = \"\"\n",
        "\n",
        "    rc = row.get(\"review_count\")\n",
        "    sm = row.get(\"score_mean\")\n",
        "    s_med = row.get(\"score_median\")\n",
        "\n",
        "    rc_txt = f\"{int(rc)}\" if pd.notna(rc) else \"0\"\n",
        "    mean_txt = f\"{float(sm):.2f}\" if pd.notna(sm) else \"NA\"\n",
        "    med_txt = f\"{float(s_med):.2f}\" if pd.notna(s_med) else \"NA\"\n",
        "\n",
        "    rv = reviews_for_snippets.loc[reviews_for_snippets[\"volume_url\"] == vol_url].copy()\n",
        "    snippets = []\n",
        "    if len(rv):\n",
        "        rv[\"snippet_len\"] = rv[\"rag_text\"].str.len()\n",
        "        rv = rv.sort_values(by=[\"review_score\",\"snippet_len\"], ascending=[False, False]).head(3)\n",
        "        for t in rv[\"rag_text\"].tolist():\n",
        "            t = re.sub(r\"\\s+\", \" \", t.strip().replace(\"\\n\", \" \"))\n",
        "            snippets.append(t[:420])\n",
        "\n",
        "    header = vol_title if vol_title else (f\"Volume {int(vol_num)}\" if pd.notna(vol_num) else \"Volume\")\n",
        "    if pd.notna(vol_num) and vol_title:\n",
        "        header = f\"{vol_title} (Tome {int(vol_num)})\"\n",
        "\n",
        "    parts = [header]\n",
        "    if vol_syn:\n",
        "        parts.append(f\"Synopsis (volume) : {vol_syn}\")\n",
        "    elif syn_series:\n",
        "        parts.append(f\"Synopsis (série) : {syn_series}\")\n",
        "    parts.append(f\"Notes (reviews) : moyenne {mean_txt} / médiane {med_txt} — {rc_txt} avis\")\n",
        "\n",
        "    if snippets:\n",
        "        parts.append(\"Extraits d'avis :\")\n",
        "        for sn in snippets:\n",
        "            parts.append(f\"- {sn}\")\n",
        "\n",
        "    return \"\\n\\n\".join(parts).strip()\n",
        "\n",
        "vdocs = []\n",
        "for _, row in ms_volumes_enriched.iterrows():\n",
        "    vol_url = safe_str(row.get(\"volume_url\")).strip()\n",
        "    if not vol_url:\n",
        "        continue\n",
        "    text = build_volume_profile(row)\n",
        "    md_ = {\n",
        "        \"volume_url\": vol_url,\n",
        "        \"series_id\": None if pd.isna(row.get(\"series_id\")) else int(row.get(\"series_id\")),\n",
        "        \"volume_number\": None if pd.isna(row.get(\"volume_number\")) else int(row.get(\"volume_number\")),\n",
        "        \"review_count\": 0 if pd.isna(row.get(\"review_count\")) else int(row.get(\"review_count\")),\n",
        "        \"score_mean\": None if pd.isna(row.get(\"score_mean\")) else float(row.get(\"score_mean\")),\n",
        "        \"score_median\": None if pd.isna(row.get(\"score_median\")) else float(row.get(\"score_median\")),\n",
        "    }\n",
        "    vdocs.append({\"id\": vol_url, \"text\": text, \"metadata\": md_})\n",
        "\n",
        "with RAG_VPROF_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for d in vdocs:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"rag_volume_profiles_written =\", len(vdocs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ea9621",
      "metadata": {},
      "source": [
        "## 8) Stats \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd94e02",
      "metadata": {},
      "outputs": [],
      "source": [
        "stats = {\n",
        "    \"rows\": {\n",
        "        \"series\": int(len(ms_series)),\n",
        "        \"volumes\": int(len(ms_volumes)),\n",
        "        \"reviews\": int(len(ms_reviews)),\n",
        "        \"reviews_rag_ready\": int(len(rag_df)),\n",
        "    },\n",
        "    \"orphans\": {\n",
        "        \"orphan_reviews_rows\": int(len(orphans)),\n",
        "        \"orphan_reviews_csv\": try_rel(ORPHAN_REVIEWS_CSV) if len(orphans) else None,\n",
        "    },\n",
        "    \"exports\": {\n",
        "        \"ms_series_enriched_csv\": try_rel(SERIES_ENR_CSV),\n",
        "        \"ms_volumes_enriched_csv\": try_rel(VOLUMES_ENR_CSV),\n",
        "        \"ms_series_metrics_csv\": try_rel(SERIES_MET_CSV),\n",
        "        \"ms_volume_metrics_csv\": try_rel(VOLUMES_MET_CSV),\n",
        "        \"rag_reviews_jsonl\": try_rel(RAG_REVIEWS_JSONL),\n",
        "        \"rag_volume_profiles_jsonl\": try_rel(RAG_VPROF_JSONL),\n",
        "    },\n",
        "}\n",
        "STATS_JSON.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "stats\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
