{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C3 — Agrégation & exports RAG-friendly (version modifiée)\n",
        "\n",
        "Modifs :\n",
        "- Suppression de `volume_members_rating` (et variante `volume_membrers_rating`) : pas utile pour ton conseiller.\n",
        "- `review_count` dans `ms_volumes_enriched` casté en **Int64** et rempli à **0** pour les volumes sans review.\n",
        "\n",
        "Entrées (`out_ms_staging/`) :\n",
        "- `ms_series_clean.csv`\n",
        "- `ms_volumes_clean.csv`\n",
        "- `ms_reviews_clean.csv`\n",
        "- (optionnel) `ms_reviews_rag_ready.csv`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed48fd24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39de198",
      "metadata": {},
      "source": [
        "## 0) Parquet (CSV + Parquet systématiques)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "41737679",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PARQUET_READY = True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "def ensure_pyarrow_or_fail():\n",
        "    try:\n",
        "        import pyarrow  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow\"])\n",
        "        import pyarrow  # noqa: F401\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"Parquet requis mais pyarrow n'est pas disponible. \"\n",
        "            \"Installe-le manuellement: pip install pyarrow. \"\n",
        "            f\"Détail: {repr(e)}\"\n",
        "        )\n",
        "\n",
        "PARQUET_READY = ensure_pyarrow_or_fail()\n",
        "print(\"PARQUET_READY =\", PARQUET_READY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85415721",
      "metadata": {},
      "source": [
        "## 1) Chemins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "968aef77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT = /home/maxime/python/certification/preparation_bdd\n",
            "STAGING = /home/maxime/python/certification/preparation_bdd/out_ms_staging\n",
            "OUT_DIR = /home/maxime/python/certification/preparation_bdd/out_ms_final\n"
          ]
        }
      ],
      "source": [
        "REQUIRED_STAGING_FILES = [\n",
        "    \"ms_series_clean.csv\",\n",
        "    \"ms_volumes_clean.csv\",\n",
        "    \"ms_reviews_clean.csv\",\n",
        "]\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for root in [start, *start.parents]:\n",
        "        staging = root / \"out_ms_staging\"\n",
        "        if all((staging / f).exists() for f in REQUIRED_STAGING_FILES):\n",
        "            return root\n",
        "    return start\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "\n",
        "STAGING = PROJECT_ROOT / \"out_ms_staging\"\n",
        "if not STAGING.exists() or not all((STAGING / f).exists() for f in REQUIRED_STAGING_FILES):\n",
        "    if Path(\"/mnt/data\").exists():\n",
        "        STAGING = Path(\"/mnt/data\")\n",
        "\n",
        "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
        "print(\"STAGING =\", STAGING)\n",
        "\n",
        "SERIES_CSV  = STAGING / \"ms_series_clean.csv\"\n",
        "VOLUMES_CSV = STAGING / \"ms_volumes_clean.csv\"\n",
        "REVIEWS_CSV = STAGING / \"ms_reviews_clean.csv\"\n",
        "RAGREV_CSV  = STAGING / \"ms_reviews_rag_ready.csv\"  # optionnel\n",
        "\n",
        "missing = [p for p in [SERIES_CSV, VOLUMES_CSV, REVIEWS_CSV] if not p.exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(f\"Fichiers manquants dans {STAGING}: {missing}\")\n",
        "\n",
        "OUT_DIR = PROJECT_ROOT / \"out_ms_final\"\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "SERIES_ENR_CSV   = OUT_DIR / \"ms_series_enriched.csv\"\n",
        "VOLUMES_ENR_CSV  = OUT_DIR / \"ms_volumes_enriched.csv\"\n",
        "SERIES_MET_CSV   = OUT_DIR / \"ms_series_metrics.csv\"\n",
        "VOLUMES_MET_CSV  = OUT_DIR / \"ms_volume_metrics.csv\"\n",
        "\n",
        "SERIES_ENR_PARQ  = OUT_DIR / \"ms_series_enriched.parquet\"\n",
        "VOLUMES_ENR_PARQ = OUT_DIR / \"ms_volumes_enriched.parquet\"\n",
        "SERIES_MET_PARQ  = OUT_DIR / \"ms_series_metrics.parquet\"\n",
        "VOLUMES_MET_PARQ = OUT_DIR / \"ms_volume_metrics.parquet\"\n",
        "\n",
        "ORPHAN_REVIEWS_CSV = OUT_DIR / \"ms_reviews_orphan_volume_url.csv\"\n",
        "STATS_JSON         = OUT_DIR / \"ms_c3_stats.json\"\n",
        "\n",
        "RAG_REVIEWS_JSONL  = OUT_DIR / \"rag_reviews.jsonl\"\n",
        "RAG_VPROF_JSONL    = OUT_DIR / \"rag_volume_profiles.jsonl\"\n",
        "\n",
        "print(\"OUT_DIR =\", OUT_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512d0764",
      "metadata": {},
      "source": [
        "## 2) Chargement (nullable dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "628204b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ms_series : (13208, 23)\n",
            "ms_volumes: (89129, 19)\n",
            "ms_reviews: (6749, 19)\n"
          ]
        }
      ],
      "source": [
        "def read_csv_nullable(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, dtype_backend=\"numpy_nullable\")\n",
        "\n",
        "ms_series  = read_csv_nullable(SERIES_CSV)\n",
        "ms_volumes = read_csv_nullable(VOLUMES_CSV)\n",
        "ms_reviews = read_csv_nullable(REVIEWS_CSV)\n",
        "\n",
        "print(\"ms_series :\", ms_series.shape)\n",
        "print(\"ms_volumes:\", ms_volumes.shape)\n",
        "print(\"ms_reviews:\", ms_reviews.shape)\n",
        "\n",
        "assert ms_series[\"series_id\"].isna().sum() == 0\n",
        "assert ms_volumes[\"volume_url\"].isna().sum() == 0\n",
        "assert ms_reviews[\"volume_url\"].isna().sum() == 0\n",
        "\n",
        "assert ms_series[\"series_id\"].is_unique\n",
        "assert ms_volumes[\"volume_url\"].is_unique\n",
        "assert ms_reviews[\"review_url\"].is_unique\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77be7693",
      "metadata": {},
      "source": [
        "## 3) Audit orphelins (volume_url absent des volumes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "571bdecb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "orphan_reviews_rows = 0\n"
          ]
        }
      ],
      "source": [
        "orphans = ms_reviews.loc[~ms_reviews[\"volume_url\"].isin(ms_volumes[\"volume_url\"])].copy()\n",
        "print(\"orphan_reviews_rows =\", len(orphans))\n",
        "if len(orphans):\n",
        "    orphans.to_csv(ORPHAN_REVIEWS_CSV, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f706311",
      "metadata": {},
      "source": [
        "## 4) KPI par volume & par série\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "13b59c6a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(                                          volume_url  review_count  \\\n",
              " 0  https://www.manga-sanctuary.com/manga-008-appr...             1   \n",
              " 1  https://www.manga-sanctuary.com/manga-008-appr...             1   \n",
              " 2  https://www.manga-sanctuary.com/manga-008-appr...             1   \n",
              " \n",
              "    score_mean  score_median  score_min  score_max  with_body_count  \\\n",
              " 0         8.0           8.0        8.0        8.0                1   \n",
              " 1         7.0           7.0        7.0        7.0                1   \n",
              " 2         6.0           6.0        6.0        6.0                1   \n",
              " \n",
              "    with_date_count first_review_date last_review_date  with_body_pct  \\\n",
              " 0                1        2021-06-13       2021-06-13          100.0   \n",
              " 1                0               NaT              NaT          100.0   \n",
              " 2                1        2025-06-22       2025-06-22          100.0   \n",
              " \n",
              "    with_date_pct first_review_date_iso last_review_date_iso  \n",
              " 0          100.0            2021-06-13           2021-06-13  \n",
              " 1            0.0                   NaN                  NaN  \n",
              " 2          100.0            2025-06-22           2025-06-22  ,\n",
              "    series_id  series_review_count  series_score_mean  series_score_median  \\\n",
              " 0         69                    2               8.75                 8.75   \n",
              " 1        417                    1                9.0                  9.0   \n",
              " 2        439                   20                8.7                  9.0   \n",
              " \n",
              "    series_score_min  series_score_max  series_with_body_count  \\\n",
              " 0               8.5               9.0                       0   \n",
              " 1               9.0               9.0                       0   \n",
              " 2               7.0              10.0                       0   \n",
              " \n",
              "    series_with_date_count series_first_review_date series_last_review_date  \\\n",
              " 0                       2               2008-01-25              2008-01-25   \n",
              " 1                       1               2006-10-27              2006-10-27   \n",
              " 2                      14               2014-11-24              2015-04-25   \n",
              " \n",
              "    series_with_body_pct  series_with_date_pct series_first_review_date_iso  \\\n",
              " 0                   0.0                 100.0                   2008-01-25   \n",
              " 1                   0.0                 100.0                   2006-10-27   \n",
              " 2                   0.0                  70.0                   2014-11-24   \n",
              " \n",
              "   series_last_review_date_iso  \n",
              " 0                  2008-01-25  \n",
              " 1                  2006-10-27  \n",
              " 2                  2015-04-25  )"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ms_reviews[\"review_score\"] = pd.to_numeric(ms_reviews.get(\"review_score\"), errors=\"coerce\").astype(\"Float64\")\n",
        "ms_reviews[\"review_date_dt\"] = pd.to_datetime(ms_reviews.get(\"review_date_iso\"), errors=\"coerce\")\n",
        "ms_reviews[\"has_date\"] = ms_reviews[\"review_date_dt\"].notna()\n",
        "ms_reviews[\"has_body\"] = ms_reviews.get(\"review_body\").notna() & (ms_reviews.get(\"review_body\").astype(str).str.strip() != \"\")\n",
        "\n",
        "g = ms_reviews.groupby(\"volume_url\", dropna=False)\n",
        "volume_metrics = g.agg(\n",
        "    review_count=(\"review_url\", \"count\"),\n",
        "    score_mean=(\"review_score\", \"mean\"),\n",
        "    score_median=(\"review_score\", \"median\"),\n",
        "    score_min=(\"review_score\", \"min\"),\n",
        "    score_max=(\"review_score\", \"max\"),\n",
        "    with_body_count=(\"has_body\", \"sum\"),\n",
        "    with_date_count=(\"has_date\", \"sum\"),\n",
        "    first_review_date=(\"review_date_dt\", \"min\"),\n",
        "    last_review_date=(\"review_date_dt\", \"max\"),\n",
        ").reset_index()\n",
        "\n",
        "volume_metrics[\"with_body_pct\"] = (volume_metrics[\"with_body_count\"] / volume_metrics[\"review_count\"] * 100).round(2)\n",
        "volume_metrics[\"with_date_pct\"] = (volume_metrics[\"with_date_count\"] / volume_metrics[\"review_count\"] * 100).round(2)\n",
        "volume_metrics[\"first_review_date_iso\"] = volume_metrics[\"first_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "volume_metrics[\"last_review_date_iso\"]  = volume_metrics[\"last_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "gs = ms_reviews.groupby(\"series_id\", dropna=False)\n",
        "series_metrics = gs.agg(\n",
        "    series_review_count=(\"review_url\", \"count\"),\n",
        "    series_score_mean=(\"review_score\", \"mean\"),\n",
        "    series_score_median=(\"review_score\", \"median\"),\n",
        "    series_score_min=(\"review_score\", \"min\"),\n",
        "    series_score_max=(\"review_score\", \"max\"),\n",
        "    series_with_body_count=(\"has_body\", \"sum\"),\n",
        "    series_with_date_count=(\"has_date\", \"sum\"),\n",
        "    series_first_review_date=(\"review_date_dt\", \"min\"),\n",
        "    series_last_review_date=(\"review_date_dt\", \"max\"),\n",
        ").reset_index()\n",
        "\n",
        "series_metrics[\"series_with_body_pct\"] = (series_metrics[\"series_with_body_count\"] / series_metrics[\"series_review_count\"] * 100).round(2)\n",
        "series_metrics[\"series_with_date_pct\"] = (series_metrics[\"series_with_date_count\"] / series_metrics[\"series_review_count\"] * 100).round(2)\n",
        "series_metrics[\"series_first_review_date_iso\"] = series_metrics[\"series_first_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "series_metrics[\"series_last_review_date_iso\"]  = series_metrics[\"series_last_review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "volume_metrics.head(3), series_metrics.head(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a5c166",
      "metadata": {},
      "source": [
        "## 5) Enrichissement + suppression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "27fb28ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped volume columns: ['volume_members_rating']\n",
            "review_count       Int64\n",
            "with_body_count    Int64\n",
            "with_date_count    Int64\n",
            "dtype: object\n",
            "series_review_count       Int64\n",
            "series_with_body_count    Int64\n",
            "series_with_date_count    Int64\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>series_id</th>\n",
              "      <th>series_review_count</th>\n",
              "      <th>series_volume_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78152</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12139</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15537</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57120</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10066</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   series_id  series_review_count  series_volume_count\n",
              "0      78152                    0                    1\n",
              "1      12139                    0                    5\n",
              "2      15537                    0                    1\n",
              "3      57120                    0                    1\n",
              "4      10066                    0                    1"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# volumes enrichis\n",
        "ms_volumes_enriched = ms_volumes.merge(\n",
        "    volume_metrics.drop(columns=[\"first_review_date\", \"last_review_date\"]),\n",
        "    on=\"volume_url\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "# ✅ suppression colonne(s) non pertinente(s) (orthographes / variations possibles)\n",
        "drop_cols = []\n",
        "for c in ms_volumes_enriched.columns:\n",
        "    cl = str(c).strip().lower()\n",
        "    if cl in {\"volume_members_rating\", \"volume_membrers_rating\"}:\n",
        "        drop_cols.append(c)\n",
        "    elif \"members_rating\" in cl or \"membrers_rating\" in cl:\n",
        "        drop_cols.append(c)\n",
        "\n",
        "drop_cols = sorted(set(drop_cols))\n",
        "if drop_cols:\n",
        "    ms_volumes_enriched = ms_volumes_enriched.drop(columns=drop_cols)\n",
        "print(\"Dropped volume columns:\", drop_cols)\n",
        "\n",
        "# ✅ compteurs volumes -> Int64 + fill 0 (volumes sans reviews)\n",
        "for c in [\"review_count\", \"with_body_count\", \"with_date_count\"]:\n",
        "    if c in ms_volumes_enriched.columns:\n",
        "        ms_volumes_enriched[c] = (\n",
        "            pd.to_numeric(ms_volumes_enriched[c], errors=\"coerce\")\n",
        "              .fillna(0)\n",
        "              .astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "print(ms_volumes_enriched[[\"review_count\", \"with_body_count\", \"with_date_count\"]].dtypes)\n",
        "ms_volumes_enriched[[\"volume_url\", \"series_id\", \"volume_number\", \"review_count\"]].head(5)\n",
        "\n",
        "# séries enrichies\n",
        "vol_count = ms_volumes.groupby(\"series_id\").size().reset_index(name=\"series_volume_count\")\n",
        "ms_series_enriched = (\n",
        "    ms_series\n",
        "    .merge(vol_count, on=\"series_id\", how=\"left\")\n",
        "    .merge(\n",
        "        series_metrics.drop(columns=[\"series_first_review_date\", \"series_last_review_date\"]),\n",
        "        on=\"series_id\",\n",
        "        how=\"left\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# ✅ compteurs KPI série -> Int64 + fill 0 (séries sans reviews)\n",
        "for c in [\"series_review_count\", \"series_with_body_count\", \"series_with_date_count\"]:\n",
        "    if c in ms_series_enriched.columns:\n",
        "        ms_series_enriched[c] = (\n",
        "            pd.to_numeric(ms_series_enriched[c], errors=\"coerce\")\n",
        "              .fillna(0)\n",
        "              .astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "print(ms_series_enriched[[\"series_review_count\", \"series_with_body_count\", \"series_with_date_count\"]].dtypes)\n",
        "ms_series_enriched[[\"series_id\", \"series_review_count\", \"series_volume_count\"]].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67902373",
      "metadata": {},
      "source": [
        "### gestion float64 / int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9ebf231c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "for c in [\"series_year\", \"series_category_year_guess\"]:\n",
        "    if c in ms_series_enriched.columns:\n",
        "        ms_series_enriched[c] = pd.to_numeric(ms_series_enriched[c], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "if \"series_review_count\" in ms_series_enriched.columns:\n",
        "    ms_series_enriched[\"series_review_count\"] = (\n",
        "        pd.to_numeric(ms_series_enriched[\"series_review_count\"], errors=\"coerce\")\n",
        "          .fillna(0)\n",
        "          .astype(\"Int64\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53478d29",
      "metadata": {},
      "source": [
        "## 6) Exports (CSV + Parquet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "25504baa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported: ms_volumes_enriched.csv ms_series_enriched.csv ms_volume_metrics.csv ms_series_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "def export_both(df: pd.DataFrame, csv_path: Path, parq_path: Path):\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_parquet(parq_path, index=False)\n",
        "\n",
        "export_both(ms_volumes_enriched, VOLUMES_ENR_CSV, VOLUMES_ENR_PARQ)\n",
        "export_both(ms_series_enriched, SERIES_ENR_CSV, SERIES_ENR_PARQ)\n",
        "export_both(volume_metrics, VOLUMES_MET_CSV, VOLUMES_MET_PARQ)\n",
        "export_both(series_metrics, SERIES_MET_CSV, SERIES_MET_PARQ)\n",
        "\n",
        "print(\"Exported:\", VOLUMES_ENR_CSV.name, SERIES_ENR_CSV.name, VOLUMES_MET_CSV.name, SERIES_MET_CSV.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec0d3e7",
      "metadata": {},
      "source": [
        "## 7) RAG docs (reviews + volume profiles) — chunking optionnel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cfd9934d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rag_reviews_docs_written = 3187\n",
            "rag_volume_profiles_written = 89129\n"
          ]
        }
      ],
      "source": [
        "USE_CHUNKING = False\n",
        "CHUNK_CHARS = 1200\n",
        "OVERLAP_CHARS = 200\n",
        "\n",
        "def chunk_text(s: str, chunk_chars=1200, overlap=200):\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return []\n",
        "    if len(s) <= chunk_chars:\n",
        "        return [s]\n",
        "    out, start = [], 0\n",
        "    while start < len(s):\n",
        "        end = min(len(s), start + chunk_chars)\n",
        "        out.append(s[start:end])\n",
        "        if end == len(s):\n",
        "            break\n",
        "        start = max(0, end - overlap)\n",
        "    return out\n",
        "\n",
        "rag_df = read_csv_nullable(RAGREV_CSV) if RAGREV_CSV.exists() else ms_reviews.loc[ms_reviews.get(\"rag_ready\") == True].copy()\n",
        "\n",
        "need_cols = [\"review_url\",\"series_id\",\"volume_url\",\"volume_number\",\"review_score\",\"review_date_iso\",\"review_author\",\"review_type\",\"rag_text\",\"rag_len\"]\n",
        "for c in need_cols:\n",
        "    if c not in rag_df.columns:\n",
        "        rag_df[c] = pd.NA\n",
        "\n",
        "# rag_reviews.jsonl\n",
        "rag_docs = []\n",
        "for _, r in rag_df.iterrows():\n",
        "    rid = str(r[\"review_url\"])\n",
        "    text = r[\"rag_text\"]\n",
        "    if text is pd.NA or str(text).strip() == \"\":\n",
        "        continue\n",
        "    chunks = chunk_text(str(text), CHUNK_CHARS, OVERLAP_CHARS) if USE_CHUNKING else [str(text)]\n",
        "    for ci, chunk in enumerate(chunks):\n",
        "        doc_id = f\"{rid}#c{ci}\" if USE_CHUNKING else rid\n",
        "        md_ = {\n",
        "            \"review_url\": rid,\n",
        "            \"series_id\": None if pd.isna(r[\"series_id\"]) else int(r[\"series_id\"]),\n",
        "            \"volume_url\": None if pd.isna(r[\"volume_url\"]) else str(r[\"volume_url\"]),\n",
        "            \"volume_number\": None if pd.isna(r[\"volume_number\"]) else int(r[\"volume_number\"]),\n",
        "            \"review_score\": None if pd.isna(r[\"review_score\"]) else float(r[\"review_score\"]),\n",
        "            \"review_date_iso\": None if pd.isna(r[\"review_date_iso\"]) else str(r[\"review_date_iso\"]),\n",
        "            \"review_author\": None if pd.isna(r[\"review_author\"]) else str(r[\"review_author\"]),\n",
        "            \"review_type\": None if pd.isna(r[\"review_type\"]) else str(r[\"review_type\"]),\n",
        "            \"chunk_index\": ci if USE_CHUNKING else None,\n",
        "        }\n",
        "        rag_docs.append({\"id\": doc_id, \"text\": chunk, \"metadata\": md_})\n",
        "\n",
        "with RAG_REVIEWS_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for d in rag_docs:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"rag_reviews_docs_written =\", len(rag_docs))\n",
        "\n",
        "# rag_volume_profiles.jsonl\n",
        "reviews_for_snippets = rag_df.copy()\n",
        "reviews_for_snippets[\"review_score\"] = pd.to_numeric(reviews_for_snippets[\"review_score\"], errors=\"coerce\").astype(\"Float64\")\n",
        "reviews_for_snippets[\"rag_text\"] = reviews_for_snippets[\"rag_text\"].astype(str)\n",
        "\n",
        "def safe_str(v) -> str:\n",
        "    if v is None:\n",
        "        return \"\"\n",
        "    try:\n",
        "        if pd.isna(v):\n",
        "            return \"\"\n",
        "    except Exception:\n",
        "        return str(v)\n",
        "    return str(v)\n",
        "\n",
        "def build_volume_profile(row):\n",
        "    vol_title = safe_str(row.get(\"volume_title\")).strip()\n",
        "    vol_num = row.get(\"volume_number\")\n",
        "    series_id = row.get(\"series_id\")\n",
        "    vol_url = safe_str(row.get(\"volume_url\")).strip()\n",
        "\n",
        "    vol_syn = safe_str(row.get(\"volume_synopsis\")).strip()\n",
        "    if vol_syn.lower() in {\"nan\", \"none\"}:\n",
        "        vol_syn = \"\"\n",
        "\n",
        "    syn_series = \"\"\n",
        "    if pd.notna(series_id):\n",
        "        srow = ms_series_enriched.loc[ms_series_enriched[\"series_id\"] == series_id]\n",
        "        if len(srow):\n",
        "            syn_series = safe_str(srow.iloc[0].get(\"series_synopsis\")).strip()\n",
        "            if syn_series.lower() in {\"nan\", \"none\"}:\n",
        "                syn_series = \"\"\n",
        "\n",
        "    rc = row.get(\"review_count\")\n",
        "    sm = row.get(\"score_mean\")\n",
        "    s_med = row.get(\"score_median\")\n",
        "\n",
        "    rc_txt = f\"{int(rc)}\" if pd.notna(rc) else \"0\"\n",
        "    mean_txt = f\"{float(sm):.2f}\" if pd.notna(sm) else \"NA\"\n",
        "    med_txt = f\"{float(s_med):.2f}\" if pd.notna(s_med) else \"NA\"\n",
        "\n",
        "    rv = reviews_for_snippets.loc[reviews_for_snippets[\"volume_url\"] == vol_url].copy()\n",
        "    snippets = []\n",
        "    if len(rv):\n",
        "        rv[\"snippet_len\"] = rv[\"rag_text\"].str.len()\n",
        "        rv = rv.sort_values(by=[\"review_score\",\"snippet_len\"], ascending=[False, False]).head(3)\n",
        "        for t in rv[\"rag_text\"].tolist():\n",
        "            t = re.sub(r\"\\s+\", \" \", t.strip().replace(\"\\n\", \" \"))\n",
        "            snippets.append(t[:420])\n",
        "\n",
        "    header = vol_title if vol_title else (f\"Volume {int(vol_num)}\" if pd.notna(vol_num) else \"Volume\")\n",
        "    if pd.notna(vol_num) and vol_title:\n",
        "        header = f\"{vol_title} (Tome {int(vol_num)})\"\n",
        "\n",
        "    parts = [header]\n",
        "    if vol_syn:\n",
        "        parts.append(f\"Synopsis (volume) : {vol_syn}\")\n",
        "    elif syn_series:\n",
        "        parts.append(f\"Synopsis (série) : {syn_series}\")\n",
        "    parts.append(f\"Notes (reviews) : moyenne {mean_txt} / médiane {med_txt} — {rc_txt} avis\")\n",
        "\n",
        "    if snippets:\n",
        "        parts.append(\"Extraits d'avis :\")\n",
        "        for sn in snippets:\n",
        "            parts.append(f\"- {sn}\")\n",
        "\n",
        "    return \"\\n\\n\".join(parts).strip()\n",
        "\n",
        "vdocs = []\n",
        "for _, row in ms_volumes_enriched.iterrows():\n",
        "    vol_url = safe_str(row.get(\"volume_url\")).strip()\n",
        "    if not vol_url:\n",
        "        continue\n",
        "    text = build_volume_profile(row)\n",
        "    md_ = {\n",
        "        \"volume_url\": vol_url,\n",
        "        \"series_id\": None if pd.isna(row.get(\"series_id\")) else int(row.get(\"series_id\")),\n",
        "        \"volume_number\": None if pd.isna(row.get(\"volume_number\")) else int(row.get(\"volume_number\")),\n",
        "        \"review_count\": 0 if pd.isna(row.get(\"review_count\")) else int(row.get(\"review_count\")),\n",
        "        \"score_mean\": None if pd.isna(row.get(\"score_mean\")) else float(row.get(\"score_mean\")),\n",
        "        \"score_median\": None if pd.isna(row.get(\"score_median\")) else float(row.get(\"score_median\")),\n",
        "    }\n",
        "    vdocs.append({\"id\": vol_url, \"text\": text, \"metadata\": md_})\n",
        "\n",
        "with RAG_VPROF_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for d in vdocs:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"rag_volume_profiles_written =\", len(vdocs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ea9621",
      "metadata": {},
      "source": [
        "## 8) Stats \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bbd94e02",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rows': {'series': 13208,\n",
              "  'volumes': 89129,\n",
              "  'reviews': 6749,\n",
              "  'reviews_rag_ready': 3187},\n",
              " 'orphans': {'orphan_reviews_rows': 0, 'orphan_reviews_csv': None},\n",
              " 'exports': {'ms_series_enriched_csv': '/home/maxime/python/certification/preparation_bdd/out_ms_final/ms_series_enriched.csv',\n",
              "  'ms_volumes_enriched_csv': '/home/maxime/python/certification/preparation_bdd/out_ms_final/ms_volumes_enriched.csv',\n",
              "  'ms_series_metrics_csv': '/home/maxime/python/certification/preparation_bdd/out_ms_final/ms_series_metrics.csv',\n",
              "  'ms_volume_metrics_csv': '/home/maxime/python/certification/preparation_bdd/out_ms_final/ms_volume_metrics.csv',\n",
              "  'rag_reviews_jsonl': '/home/maxime/python/certification/preparation_bdd/out_ms_final/rag_reviews.jsonl',\n",
              "  'rag_volume_profiles_jsonl': '/home/maxime/python/certification/preparation_bdd/out_ms_final/rag_volume_profiles.jsonl'}}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats = {\n",
        "    \"rows\": {\n",
        "        \"series\": int(len(ms_series)),\n",
        "        \"volumes\": int(len(ms_volumes)),\n",
        "        \"reviews\": int(len(ms_reviews)),\n",
        "        \"reviews_rag_ready\": int(len(rag_df)),\n",
        "    },\n",
        "    \"orphans\": {\n",
        "        \"orphan_reviews_rows\": int(len(orphans)),\n",
        "        \"orphan_reviews_csv\": str(ORPHAN_REVIEWS_CSV) if len(orphans) else None,\n",
        "    },\n",
        "    \"exports\": {\n",
        "        \"ms_series_enriched_csv\": str(SERIES_ENR_CSV),\n",
        "        \"ms_volumes_enriched_csv\": str(VOLUMES_ENR_CSV),\n",
        "        \"ms_series_metrics_csv\": str(SERIES_MET_CSV),\n",
        "        \"ms_volume_metrics_csv\": str(VOLUMES_MET_CSV),\n",
        "        \"rag_reviews_jsonl\": str(RAG_REVIEWS_JSONL),\n",
        "        \"rag_volume_profiles_jsonl\": str(RAG_VPROF_JSONL),\n",
        "    },\n",
        "}\n",
        "STATS_JSON.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "stats\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
