{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc9baac",
   "metadata": {},
   "source": [
    "# PrÃ©paration des exports hebdomadaires Kitsu â†’ PostgreSQL (projet RAG)\n",
    "\n",
    "Ce notebook prend **3 fichiers JSON hebdomadaires** (Kitsu) :\n",
    "\n",
	    "- `most_popular.json`\n",
	    "- `top_publishing.json`\n",
	    "- `trending_weekly.json`\n",
	    "\n",
	    "> DonnÃ©es : `Preparation_weekly/data/`.\n",
	    "> Exports : `Preparation_weekly/export/`.\n",
	    "\n",
	    "Objectifs :\n",
    "\n",
    "1. **ContrÃ´les qualitÃ©** (doublons, valeurs manquantes, cohÃ©rence des types)\n",
    "2. **Nettoyage / normalisation** (synopsis, titres, tags)\n",
    "3. GÃ©nÃ©ration de tables prÃªtes Ã  charger dans **PostgreSQL** pour :\n",
    "   - un rÃ©fÃ©rentiel `kitsu_series_core` (1 ligne par manga/serie Kitsu)\n",
    "   - un historique `kitsu_weekly_snapshot` (1 ligne par sÃ©rie et par liste/sem.)\n",
    "   - une table `kitsu_series_authors` (optionnelle mais utile pour RAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionnel) Installer les dÃ©pendances si nÃ©cessaire\n",
    "# !pip install -q pandas python-dateutil unidecode sqlalchemy psycopg2-binary\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dateutil import parser as dtparser\n",
    "from unidecode import unidecode\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ ParamÃ¨tres / chemins (tout reste dans `Preparation_weekly/`)\n",
    "REPO_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p / \"pyproject.toml\").exists()), Path.cwd())\n",
    "WEEKLY_DIR = REPO_ROOT / \"Preparation_weekly\"\n",
    "DATA_DIR = WEEKLY_DIR / \"data\"\n",
    "OUT_DIR = WEEKLY_DIR / \"export\"  # dossier dÃ©jÃ  prÃ©sent\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FILES = {\n",
    "    \"most_popular\": DATA_DIR / \"most_popular.json\",\n",
    "    \"top_publishing\": DATA_DIR / \"top_publishing.json\",\n",
    "    \"trending_weekly\": DATA_DIR / \"trending_weekly.json\",\n",
    "}\n",
    "\n",
    "missing = [k for k,p in FILES.items() if not p.exists()]\n",
    "if missing:\n",
    "    print(\"âš ï¸ Fichiers manquants :\", missing)\n",
    "    print(\"Placez-les dans\", DATA_DIR.resolve())\n",
    "else:\n",
    "    print(\"âœ… Tous les fichiers sont prÃ©sents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kitsu_export(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "exports = {name: load_kitsu_export(path) for name, path in FILES.items() if path.exists()}\n",
    "{k: (len(v.get(\"data\", [])), v.get(\"meta\", {}).get(\"endpoint\"), v.get(\"meta\", {}).get(\"fetched_at\")) for k,v in exports.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c49dd",
   "metadata": {},
   "source": [
    "## 1) ContrÃ´les qualitÃ© (doublons / manquants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_export(export: dict, list_name: str) -> pd.DataFrame:\n",
    "    meta = export.get(\"meta\", {}) or {}\n",
    "    fetched_at = meta.get(\"fetched_at\")\n",
    "    endpoint = meta.get(\"endpoint\")\n",
    "    category = meta.get(\"category\", list_name)\n",
    "\n",
    "    rows = []\n",
    "    for pos, it in enumerate(export.get(\"data\", []) or [], start=1):\n",
    "        titles = it.get(\"titles\") or {}\n",
    "        ratings = it.get(\"ratings\") or {}\n",
    "        pop = it.get(\"popularity\") or {}\n",
    "        tags = it.get(\"tags\") or {}\n",
    "        authors = it.get(\"authors\") or []\n",
    "\n",
    "        rows.append({\n",
    "            \"list_name\": category,\n",
    "            \"position\": pos,\n",
    "            \"fetched_at\": fetched_at,\n",
    "            \"endpoint\": endpoint,\n",
    "\n",
    "            \"kitsu_id\": it.get(\"id\"),\n",
    "            \"slug\": it.get(\"slug\"),\n",
    "            \"status\": it.get(\"status\"),\n",
    "\n",
    "            \"title_canonical\": titles.get(\"canonical\"),\n",
    "            \"title_en\": titles.get(\"en\"),\n",
    "            \"title_ja\": titles.get(\"ja\"),\n",
    "\n",
    "            \"synopsis\": it.get(\"synopsis\"),\n",
    "\n",
    "            \"rating_average\": ratings.get(\"average\"),\n",
    "            \"rating_rank\": ratings.get(\"rank\"),\n",
    "            \"popularity_rank\": pop.get(\"rank\"),\n",
    "\n",
    "            \"categories\": tags.get(\"categories\") or [],\n",
    "            \"genres\": tags.get(\"genres\") or [],\n",
    "\n",
    "            \"authors_raw\": authors,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_lists = []\n",
    "for name, export in exports.items():\n",
    "    df_lists.append(flatten_export(export, name))\n",
    "\n",
    "df_lists = pd.concat(df_lists, ignore_index=True) if df_lists else pd.DataFrame()\n",
    "df_lists.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    rep = []\n",
    "    for list_name, g in df.groupby(\"list_name\"):\n",
    "        dup = g[\"kitsu_id\"].duplicated().sum()\n",
    "        rep.append({\n",
    "            \"list_name\": list_name,\n",
    "            \"rows\": len(g),\n",
    "            \"distinct_kitsu_id\": g[\"kitsu_id\"].nunique(),\n",
    "            \"duplicate_rows\": int(dup),\n",
    "            \"pct_no_authors\": float((g[\"authors_raw\"].apply(len) == 0).mean() * 100),\n",
    "            \"pct_no_genres\": float((g[\"genres\"].apply(len) == 0).mean() * 100),\n",
    "            \"pct_missing_ja\": float(g[\"title_ja\"].isna().mean() * 100),\n",
    "        })\n",
    "    return pd.DataFrame(rep).sort_values(\"list_name\")\n",
    "\n",
    "quality_report(df_lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222e050",
   "metadata": {},
   "source": [
    "## 2) Nettoyage & normalisation\n",
    "\n",
    "On produit :\n",
    "\n",
    "- **kitsu_series_core** : 1 ligne / `kitsu_id` (dÃ©doublonnÃ©, champs nettoyÃ©s)\n",
    "- **kitsu_weekly_snapshot** : historique hebdo (liste + position + fetched_at)\n",
    "- **kitsu_series_authors** : table de liaison (optionnelle)\n",
    "\n",
    "Principes :\n",
    "\n",
    "- DÃ©doublonnage **dans une liste** par `kitsu_id` (on garde la 1Ã¨re occurrence)\n",
    "- Normalisation des titres (`title_norm_*`) pour faciliter le matching MS â†” Kitsu\n",
    "- Nettoyage synopsis (espaces, â€œ(Source: â€¦)â€ facultatif)\n",
    "- Normalisation tags : union + dÃ©doublonnage + mapping optionnel (ex: `Sci-Fi` â†’ `Science Fiction`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c87510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_text(s: str | None) -> str | None:\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s or None\n",
    "\n",
    "def clean_synopsis(s: str | None) -> str | None:\n",
    "    s = norm_text(s)\n",
    "    if not s:\n",
    "        return None\n",
    "    # Option: enlever les mentions de source Ã  la fin (souvent entre parenthÃ¨ses)\n",
    "    # -> conserve le contenu utile au RAG, enlÃ¨ve le bruit (Source: ...)\n",
    "    s = re.sub(r\"\\(Source:[^)]+\\)\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s or None\n",
    "\n",
    "def title_norm(s: str | None) -> str | None:\n",
    "    s = norm_text(s)\n",
    "    if not s:\n",
    "        return None\n",
    "    s = unidecode(s)  # supprime les accents\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or None\n",
    "\n",
    "TAG_MAP = {\n",
    "    \"sci fi\": \"science fiction\",\n",
    "    \"sci-fi\": \"science fiction\",\n",
    "    \"school\": \"school life\",   # exemple : Ã  adapter Ã  ton projet\n",
    "}\n",
    "\n",
    "def norm_tag(t: str) -> str:\n",
    "    t2 = title_norm(t) or \"\"\n",
    "    return TAG_MAP.get(t2, t.strip())\n",
    "\n",
    "def to_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dedupe_keep_first(df: pd.DataFrame, key: str) -> pd.DataFrame:\n",
    "    return df.loc[~df[key].duplicated()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 DÃ©doublonnage par liste (utile si un export contient des items rÃ©pÃ©tÃ©s)\n",
    "df_lists_clean = []\n",
    "for list_name, g in df_lists.groupby(\"list_name\"):\n",
    "    g2 = dedupe_keep_first(g, \"kitsu_id\")\n",
    "    df_lists_clean.append(g2)\n",
    "\n",
    "df_lists_clean = pd.concat(df_lists_clean, ignore_index=True)\n",
    "quality_report(df_lists_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8728218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Construire la table core (dÃ©doublonnÃ©e globalement)\n",
    "df_core = df_lists_clean.copy()\n",
    "\n",
    "# conversions de types\n",
    "df_core[\"kitsu_id\"] = df_core[\"kitsu_id\"].apply(to_int)\n",
    "df_core[\"rating_average\"] = df_core[\"rating_average\"].apply(to_float)\n",
    "df_core[\"rating_rank\"] = df_core[\"rating_rank\"].apply(to_int)\n",
    "df_core[\"popularity_rank\"] = df_core[\"popularity_rank\"].apply(to_int)\n",
    "\n",
    "# champs texte nettoyÃ©s\n",
    "for c in [\"slug\", \"status\", \"title_canonical\", \"title_en\", \"title_ja\"]:\n",
    "    df_core[c] = df_core[c].apply(norm_text)\n",
    "df_core[\"synopsis_clean\"] = df_core[\"synopsis\"].apply(clean_synopsis)\n",
    "\n",
    "# titres normalisÃ©s (matching cross-sources)\n",
    "df_core[\"title_norm_canonical\"] = df_core[\"title_canonical\"].apply(title_norm)\n",
    "df_core[\"title_norm_en\"] = df_core[\"title_en\"].apply(title_norm)\n",
    "df_core[\"title_norm_ja\"] = df_core[\"title_ja\"].apply(title_norm)\n",
    "\n",
    "# tags normalisÃ©s\n",
    "df_core[\"categories_norm\"] = df_core[\"categories\"].apply(lambda xs: sorted({norm_tag(x) for x in xs if x}))\n",
    "df_core[\"genres_norm\"] = df_core[\"genres\"].apply(lambda xs: sorted({norm_tag(x) for x in xs if x}))\n",
    "df_core[\"tags_all_norm\"] = df_core.apply(lambda r: sorted(set(r[\"categories_norm\"]) | set(r[\"genres_norm\"])), axis=1)\n",
    "\n",
    "# rating sur 10 (optionnel)\n",
    "df_core[\"rating_average_10\"] = df_core[\"rating_average\"].apply(lambda x: round(x/10, 2) if isinstance(x, (int,float)) else None)\n",
    "\n",
    "# dÃ©doublonnage global : 1 ligne par kitsu_id\n",
    "df_core = df_core.sort_values([\"fetched_at\", \"list_name\", \"position\"]).drop_duplicates(\"kitsu_id\", keep=\"first\").copy()\n",
    "\n",
    "df_core[[\"kitsu_id\",\"title_canonical\",\"rating_average\",\"rating_average_10\",\"popularity_rank\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c81700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Table weekly snapshot : on garde l'historique par liste\n",
    "df_snapshot = df_lists_clean.copy()\n",
    "df_snapshot[\"kitsu_id\"] = df_snapshot[\"kitsu_id\"].apply(to_int)\n",
    "\n",
    "# parse fetched_at -> timestamp\n",
    "def parse_ts(s):\n",
    "    try:\n",
    "        return dtparser.isoparse(s) if s else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_snapshot[\"fetched_at_ts\"] = df_snapshot[\"fetched_at\"].apply(parse_ts)\n",
    "\n",
    "df_snapshot = df_snapshot[[\n",
    "    \"list_name\",\"position\",\"fetched_at_ts\",\"endpoint\",\"kitsu_id\"\n",
    "]].copy()\n",
    "\n",
    "df_snapshot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f10d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Table auteurs (optionnelle)\n",
    "# Utile pour RAG (enrichir le doc), et pour matching (certains auteurs/roles)\n",
    "rows = []\n",
    "for _, r in df_lists_clean.iterrows():\n",
    "    kitsu_id = to_int(r[\"kitsu_id\"])\n",
    "    for a in (r[\"authors_raw\"] or []):\n",
    "        rows.append({\n",
    "            \"kitsu_id\": kitsu_id,\n",
    "            \"author_name\": norm_text(a.get(\"name\")),\n",
    "            \"author_role\": norm_text(a.get(\"role\")),\n",
    "        })\n",
    "\n",
    "df_authors = pd.DataFrame(rows).dropna(subset=[\"kitsu_id\",\"author_name\"]).drop_duplicates()\n",
    "df_authors.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b784dd",
   "metadata": {},
   "source": [
    "## 3) Exports fichiers (CSV) prÃªts pour PostgreSQL\n",
    "\n",
    "On Ã©crit 3 CSV :\n",
    "\n",
    "- `kitsu_series_core.csv`\n",
    "- `kitsu_weekly_snapshot.csv`\n",
    "- `kitsu_series_authors.csv`\n",
    "\n",
    "Ces CSV sont compatibles avec `\\copy` (psql) ou un chargement via pandas/SQLAlchemy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports CSV (pour \\copy Postgres) -> Preparation_weekly/export\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "core_path = OUT_DIR / \"kitsu_series_core.csv\"\n",
    "snapshot_path = OUT_DIR / \"kitsu_weekly_snapshot.csv\"\n",
    "authors_path = OUT_DIR / \"kitsu_series_authors.csv\"\n",
    "\n",
    "# Note : on sÃ©rialise les listes en JSON pour stocker en JSONB dans Postgres\n",
    "df_core_out = df_core.copy()\n",
    "df_core_out[\"categories_json\"] = df_core_out[\"categories_norm\"].apply(json.dumps)\n",
    "df_core_out[\"genres_json\"] = df_core_out[\"genres_norm\"].apply(json.dumps)\n",
    "df_core_out[\"tags_all_json\"] = df_core_out[\"tags_all_norm\"].apply(json.dumps)\n",
    "\n",
    "df_core_out = df_core_out[[\n",
    "    \"kitsu_id\",\"slug\",\"status\",\n",
    "    \"title_canonical\",\"title_en\",\"title_ja\",\n",
    "    \"title_norm_canonical\",\"title_norm_en\",\"title_norm_ja\",\n",
    "    \"synopsis_clean\",\n",
    "    \"rating_average\",\"rating_average_10\",\"rating_rank\",\"popularity_rank\",\n",
    "    \"categories_json\",\"genres_json\",\"tags_all_json\"\n",
    "]].copy()\n",
    "\n",
    "df_core_out.to_csv(core_path, index=False)\n",
    "df_snapshot.to_csv(snapshot_path, index=False)\n",
    "df_authors.to_csv(authors_path, index=False)\n",
    "\n",
    "print(\"âœ… CSV Ã©crits dans:\", OUT_DIR.resolve())\n",
    "print(core_path, snapshot_path, authors_path, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5473bd",
   "metadata": {},
   "source": [
    "## 4) SchÃ©ma PostgreSQL (DDL)\n",
    "\n",
    "Exemple minimal (Ã  adapter Ã  ton schÃ©ma `manga.*`).\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS manga.kitsu_series_core (\n",
    "  kitsu_id BIGINT PRIMARY KEY,\n",
    "  slug TEXT,\n",
    "  status TEXT,\n",
    "  title_canonical TEXT,\n",
    "  title_en TEXT,\n",
    "  title_ja TEXT,\n",
    "  title_norm_canonical TEXT,\n",
    "  title_norm_en TEXT,\n",
    "  title_norm_ja TEXT,\n",
    "  synopsis_clean TEXT,\n",
    "  rating_average DOUBLE PRECISION,\n",
    "  rating_average_10 DOUBLE PRECISION,\n",
    "  rating_rank INTEGER,\n",
    "  popularity_rank INTEGER,\n",
    "  categories_json JSONB,\n",
    "  genres_json JSONB,\n",
    "  tags_all_json JSONB\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS manga.kitsu_weekly_snapshot (\n",
    "  list_name TEXT NOT NULL,\n",
    "  position INTEGER NOT NULL,\n",
    "  fetched_at_ts TIMESTAMPTZ NOT NULL,\n",
    "  endpoint TEXT,\n",
    "  kitsu_id BIGINT NOT NULL REFERENCES manga.kitsu_series_core(kitsu_id),\n",
    "  PRIMARY KEY (list_name, fetched_at_ts, kitsu_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS manga.kitsu_series_authors (\n",
    "  kitsu_id BIGINT NOT NULL REFERENCES manga.kitsu_series_core(kitsu_id),\n",
    "  author_name TEXT NOT NULL,\n",
    "  author_role TEXT,\n",
    "  PRIMARY KEY (kitsu_id, author_name, author_role)\n",
    ");\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b858f4",
   "metadata": {},
   "source": [
    "## 5) Chargement PostgreSQL (2 options)\n",
    "\n",
    "### Option A â€” psql `\\copy`\n",
    "```bash\n",
    "psql -d apimanga -c \"\\copy manga.kitsu_series_core FROM 'Preparation_weekly/export/kitsu_series_core.csv' CSV HEADER\"\n",
    "psql -d apimanga -c \"\\copy manga.kitsu_weekly_snapshot FROM 'Preparation_weekly/export/kitsu_weekly_snapshot.csv' CSV HEADER\"\n",
    "psql -d apimanga -c \"\\copy manga.kitsu_series_authors FROM 'Preparation_weekly/export/kitsu_series_authors.csv' CSV HEADER\"\n",
    "```\n",
    "\n",
    "### Option B â€” SQLAlchemy (Python)\n",
    "DÃ©finir `DATABASE_URL` comme :\n",
    "`postgresql+psycopg2://user:password@host:5432/dbname`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B : insertion via SQLAlchemy (dÃ©commenter pour utiliser)\n",
    "# import os\n",
    "# from sqlalchemy import create_engine\n",
    "# DATABASE_URL = os.environ.get(\"DATABASE_URL\")\n",
    "# assert DATABASE_URL, \"DÃ©finissez DATABASE_URL\"\n",
    "# engine = create_engine(DATABASE_URL)\n",
    "#\n",
    "# # on Ã©crit dans des tables temporaires puis on upsert cÃ´tÃ© SQL si besoin\n",
    "# df_core_out.to_sql(\"kitsu_series_core\", engine, schema=\"manga\", if_exists=\"append\", index=False, method=\"multi\", chunksize=2000)\n",
    "# df_snapshot.to_sql(\"kitsu_weekly_snapshot\", engine, schema=\"manga\", if_exists=\"append\", index=False, method=\"multi\", chunksize=2000)\n",
    "# df_authors.to_sql(\"kitsu_series_authors\", engine, schema=\"manga\", if_exists=\"append\", index=False, method=\"multi\", chunksize=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332fd1d",
   "metadata": {},
   "source": [
    "## 6) Bonus RAG : construire un â€œdocumentâ€ texte par sÃ©rie\n",
    "\n",
    "IdÃ©e : une table `manga.documents_rag` ou un export parquet/csv pour lâ€™index vectoriel.\n",
    "\n",
    "Contenu typique :\n",
    "- titres (canon + EN + JA)\n",
    "- synopsis_clean\n",
    "- tags_all_norm\n",
    "- auteurs (si dispo)\n",
    "\n",
    "Le but : **retrieval** meilleur (requÃªtes utilisateur â†’ documents pertinents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_doc(row: pd.Series, authors_df: pd.DataFrame) -> str:\n",
    "    parts = []\n",
    "    titles = [row.get(\"title_canonical\"), row.get(\"title_en\"), row.get(\"title_ja\")]\n",
    "    titles = [t for t in titles if t]\n",
    "    if titles:\n",
    "        parts.append(\"Titres: \" + \" | \".join(dict.fromkeys(titles)))\n",
    "    if row.get(\"synopsis_clean\"):\n",
    "        parts.append(\"Synopsis: \" + row[\"synopsis_clean\"])\n",
    "    tags = row.get(\"tags_all_norm\") or []\n",
    "    if tags:\n",
    "        parts.append(\"Tags: \" + \", \".join(tags))\n",
    "    aid = row.get(\"kitsu_id\")\n",
    "    if aid is not None and not authors_df.empty:\n",
    "        a = authors_df[authors_df[\"kitsu_id\"] == aid]\n",
    "        if not a.empty:\n",
    "            parts.append(\"Auteurs: \" + \"; \".join((a[\"author_name\"] + (\": \" + a[\"author_role\"].fillna(\"\"))).str.strip(\": \")))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df_rag = df_core[[\"kitsu_id\",\"slug\",\"title_canonical\",\"title_en\",\"title_ja\",\"synopsis_clean\",\"tags_all_norm\"]].copy()\n",
    "df_rag[\"doc_text\"] = df_rag.apply(lambda r: build_rag_doc(r, df_authors), axis=1)\n",
    "\n",
    "rag_path = OUT_DIR / \"kitsu_rag_documents.csv\"\n",
    "df_rag.assign(tags_all_json=df_rag[\"tags_all_norm\"].apply(json.dumps)).drop(columns=[\"tags_all_norm\"]).to_csv(rag_path, index=False)\n",
    "print(\"âœ… Export RAG:\", rag_path.resolve())\n",
    "df_rag[[\"kitsu_id\",\"doc_text\"]].head(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
